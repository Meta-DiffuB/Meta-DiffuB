{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00a3796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('.')\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e4fbb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31000\n"
     ]
    }
   ],
   "source": [
    "diffusion_ver = 'metaBeta_ver'\n",
    "dataset_ = 'WA'\n",
    "checkpoint_path = f'checkpoint/{dataset_}/{diffusion_ver}'\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    print('ERROR')\n",
    "    # os.makedirs(checkpoint_path)\n",
    "    \n",
    "checkpoints = sorted(glob.glob(f\"{checkpoint_path}/model/modelD_ema_*.pt\"))[::-1]\n",
    "\n",
    "epoch = list()\n",
    "for checkpoint in checkpoints:\n",
    "    try:\n",
    "        epoch.append(int(checkpoint.split('ema_')[1].split('.pt')[0]))\n",
    "    except:\n",
    "        print(checkpoint.split('/model/')[1])\n",
    "# print(checkpoints)\n",
    "print(max(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916107cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = './datasets/WA'\n",
    "dataset = 'WA'\n",
    "diffusion_steps = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd663e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca6df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import socket\n",
    "import blobfile as bf\n",
    "import io\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import functools\n",
    "import gc\n",
    "from functools import partial\n",
    "\n",
    "# should I copy function code replace import?\n",
    "from diffuseq.utils import logger\n",
    "from diffuseq.utils.nn import (\n",
    "    SiLU,\n",
    "    linear,\n",
    "    timestep_embedding,\n",
    "    mean_flat,\n",
    "    update_ema\n",
    ")\n",
    "from diffuseq.utils.fp16_util import (\n",
    "    make_master_params,\n",
    "    master_params_to_model_params,\n",
    "    model_grads_to_master_grads,\n",
    "    unflatten_master_params,\n",
    "    zero_grad,\n",
    ")\n",
    "\n",
    "from transformers import set_seed, AutoTokenizer, PreTrainedTokenizerFast, AutoConfig\n",
    "from transformers.models.bert.modeling_bert import BertEncoder, BertModel\n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import psutil\n",
    "import datasets\n",
    "from datasets import Dataset as Dataset2\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "from itertools import chain,groupby\n",
    "\n",
    "# from torchmetrics.text.rouge import ROUGEScore\n",
    "# from bert_score import score\n",
    "# from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "# import nltk\n",
    "\n",
    "import subprocess as sp\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2971be",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPUNumber = 0\n",
    "\n",
    "seed = 5487\n",
    "\n",
    "learning_rate = 1e-4\n",
    "# batch_size = 128\n",
    "# microbatch = 64\n",
    "hidden_t_dim = 128\n",
    "hidden_dim = 128\n",
    "dropout = 0.1\n",
    "weight_decay = 0.0\n",
    "\n",
    "# learning_steps = 2000  \n",
    "\n",
    "# log_interval = 20\n",
    "# save_interval = 100 # 400\n",
    "# eval_interval = 100 # 1000\n",
    "\n",
    "# ema_rate = '0.5' # '0.9999'\n",
    "# resume_checkpoint = 'none'\n",
    "schedule_sampler_name = 'lossaware'\n",
    "noise_schedule = 'sqrt'\n",
    "timestep_respacing = ''\n",
    "\n",
    "vocab = 'bert'\n",
    "# vocab_size = 0 \n",
    "seq_len = 128\n",
    "\n",
    "use_plm_init = 'no'\n",
    "\n",
    "config_name = 'bert-base-uncased'\n",
    "notes = 'folder-notes'\n",
    "\n",
    "use_fp16 = False\n",
    "# fp16_scale_growth = 0.001\n",
    "gradient_clipping = -1.0\n",
    "learn_sigma = False\n",
    "use_kl = False\n",
    "predict_xstart = True\n",
    "rescale_timesteps = True\n",
    "rescale_learned_sigmas = False\n",
    "sigma_small = False\n",
    "emb_scale_factor = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d4488",
   "metadata": {},
   "source": [
    "## Funciton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f1c04",
   "metadata": {},
   "source": [
    "### dist_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2b340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dist():\n",
    "    \"\"\"\n",
    "    Setup a distributed process group.\n",
    "    \"\"\"\n",
    "    if dist.is_initialized():\n",
    "        return\n",
    "\n",
    "    backend = \"gloo\" if not th.cuda.is_available() else \"nccl\"\n",
    "\n",
    "    if backend == \"gloo\":\n",
    "        hostname = \"localhost\"\n",
    "    else:\n",
    "        hostname = socket.gethostbyname(socket.getfqdn())\n",
    "\n",
    "    if os.environ.get(\"LOCAL_RANK\") is None:\n",
    "        os.environ[\"MASTER_ADDR\"] = hostname\n",
    "        os.environ[\"RANK\"] = str(0)\n",
    "        os.environ[\"WORLD_SIZE\"] = str(1)\n",
    "        port = _find_free_port()\n",
    "        os.environ[\"MASTER_PORT\"] = str(port)\n",
    "        os.environ['LOCAL_RANK'] = str(GPUNumber)\n",
    "        # Bala GPU select\n",
    "        # th.cuda.set_device(GPUNumber)\n",
    "    \n",
    "    dist.init_process_group(backend=backend, init_method=\"env://\")\n",
    "    \n",
    "def dev():\n",
    "    \"\"\"\n",
    "    Get the device to use for torch.distributed.\n",
    "    \"\"\"\n",
    "    if th.cuda.is_available():\n",
    "        return th.device(f\"cuda:{os.environ['LOCAL_RANK']}\")\n",
    "    return th.device(\"cpu\")\n",
    "\n",
    "\n",
    "def load_state_dict(path, **kwargs):\n",
    "    \"\"\"\n",
    "    Load a PyTorch file.\n",
    "    \"\"\"\n",
    "    # if int(os.environ['LOCAL_RANK']) == 0:\n",
    "    with bf.BlobFile(path, \"rb\") as f:\n",
    "        data = f.read()\n",
    "    return th.load(io.BytesIO(data), **kwargs)\n",
    "\n",
    "\n",
    "def sync_params(params):\n",
    "    \"\"\"\n",
    "    Synchronize a sequence of Tensors across ranks from rank 0.\n",
    "    \"\"\"\n",
    "    for p in params:\n",
    "        with th.no_grad():\n",
    "            dist.broadcast(p, 0)\n",
    "\n",
    "\n",
    "def _find_free_port():\n",
    "    try:\n",
    "        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        s.bind((\"\", 0))\n",
    "        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        return s.getsockname()[1]\n",
    "    finally:\n",
    "        s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88387b",
   "metadata": {},
   "source": [
    "### basic_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a43f8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myTokenizer():\n",
    "    \"\"\"\n",
    "    Load tokenizer from bert config or defined BPE vocab dict\n",
    "    \"\"\"\n",
    "    ################################################\n",
    "    ### You can custome your own tokenizer here. ###\n",
    "    ################################################\n",
    "    def __init__(self, config_name, checkpoint_path, vocab):\n",
    "        if vocab == 'bert':\n",
    "            tokenizer = AutoTokenizer.from_pretrained(config_name)\n",
    "            self.tokenizer = tokenizer\n",
    "            self.sep_token_id = tokenizer.sep_token_id\n",
    "            self.pad_token_id = tokenizer.pad_token_id\n",
    "            # save\n",
    "            tokenizer.save_pretrained(checkpoint_path)\n",
    "        else: \n",
    "            # load vocab from the path\n",
    "            print('#'*30, 'load vocab from', vocab)\n",
    "            vocab_dict = {'[START]': 0, '[END]': 1, '[UNK]':2, '[PAD]':3}\n",
    "            with open(vocab, 'r', encoding='utf-8') as f:\n",
    "                for row in f:\n",
    "                    vocab_dict[row.strip().split(' ')[0]] = len(vocab_dict)\n",
    "            self.tokenizer = vocab_dict\n",
    "            self.rev_tokenizer = {v: k for k, v in vocab_dict.items()}\n",
    "            self.sep_token_id = vocab_dict['[END]']\n",
    "            self.pad_token_id = vocab_dict['[PAD]']\n",
    "            # save\n",
    "            if int(os.environ['LOCAL_RANK']) == 0:\n",
    "                path_save_vocab = f'{checkpoint_path}/vocab.json'\n",
    "                with open(path_save_vocab, 'w') as f:\n",
    "                    json.dump(vocab_dict, f)\n",
    "                \n",
    "        self.vocab_size = len(self.tokenizer)\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def encode_token(self, sentences):\n",
    "        if isinstance(self.tokenizer, dict):\n",
    "            input_ids = [[0] + [self.tokenizer.get(x, self.tokenizer['[UNK]']) for x in seq.split()] + [1] for seq in sentences]\n",
    "        elif isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            # special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
    "            input_ids = self.tokenizer(sentences, add_special_tokens=True)['input_ids']\n",
    "        else:\n",
    "            assert False, \"invalid type of vocab_dict\"\n",
    "        return input_ids\n",
    "        \n",
    "    def decode_token(self, seq):\n",
    "        if isinstance(self.tokenizer, dict):\n",
    "            seq = seq.squeeze(-1).tolist()\n",
    "            while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "                seq.pop()\n",
    "            tokens = \" \".join([self.rev_tokenizer[x] for x in seq]).replace('__ ', '').replace('@@ ', '')\n",
    "        elif isinstance(self.tokenizer, PreTrainedTokenizerFast):\n",
    "            seq = seq.squeeze(-1).tolist()\n",
    "            while len(seq)>0 and seq[-1] == self.pad_token_id:\n",
    "                seq.pop()\n",
    "            tokens = self.tokenizer.decode(seq)\n",
    "        else:\n",
    "            assert False, \"invalid type of vocab_dict\"\n",
    "        return tokens\n",
    "    \n",
    "def load_tokenizer(config_name, checkpoint_path, vocab):\n",
    "    tokenizer = myTokenizer(config_name, checkpoint_path, vocab)\n",
    "    vocab_size = tokenizer.get_vocab_size()\n",
    "    return tokenizer, vocab_size\n",
    "\n",
    "def load_model_emb(vocab_size, hidden_dim, checkpoint_path):\n",
    "    ### random emb or pre-defined embedding like glove embedding. You can custome your own init here.\n",
    "    model = th.nn.Embedding(vocab_size, hidden_dim)\n",
    "    path_save = '{}/random_emb.torch'.format(checkpoint_path)\n",
    "    path_save_ind = path_save + \".done\"\n",
    "    if int(os.environ['LOCAL_RANK']) == GPUNumber:\n",
    "        if os.path.exists(path_save):\n",
    "            print('reload the random embeddings', model)\n",
    "            model.load_state_dict(th.load(path_save))\n",
    "        else:\n",
    "            print('initializing the random embeddings', model)\n",
    "            # random Gaussian embeddings as well as pre-trained word embedding (from diffusion-LM paper)\n",
    "            th.nn.init.normal_(model.weight)\n",
    "            th.save(model.state_dict(), path_save)\n",
    "            os.sync() # It is used to force write of everything to disk.\n",
    "            with open(path_save_ind, \"x\") as _:\n",
    "                pass\n",
    "    else:\n",
    "        while not os.path.exists(path_save_ind):\n",
    "            time.sleep(1)\n",
    "        print('reload the random embeddings', model)\n",
    "        model.load_state_dict(th.load(path_save))\n",
    "\n",
    "    return model\n",
    "\n",
    "# def create_model_and_diffusion(\n",
    "#     hidden_t_dim,\n",
    "#     hidden_dim,\n",
    "#     vocab_size,\n",
    "#     config_name,\n",
    "#     use_plm_init,\n",
    "#     dropout,\n",
    "#     diffusion_steps,\n",
    "#     betas,\n",
    "#     # noise_schedule,\n",
    "#     learn_sigma,\n",
    "#     timestep_respacing,\n",
    "#     predict_xstart,\n",
    "#     rescale_timesteps,\n",
    "#     sigma_small,\n",
    "#     rescale_learned_sigmas,\n",
    "#     use_kl,\n",
    "#     notes,\n",
    "#     **kwargs,\n",
    "# ):\n",
    "#     model = TransformerNetModel(\n",
    "#         input_dims=hidden_dim,\n",
    "#         output_dims=(hidden_dim if not learn_sigma else hidden_dim*2),\n",
    "#         hidden_t_dim=hidden_t_dim,\n",
    "#         dropout=dropout,\n",
    "#         config_name=config_name,\n",
    "#         vocab_size=vocab_size,\n",
    "#         init_pretrained=use_plm_init\n",
    "#     )\n",
    "\n",
    "#     # [Bala] 把betas拆到外面sample\n",
    "#     # beta scheduler\n",
    "#     # betas = get_named_beta_schedule(noise_schedule, diffusion_steps)\n",
    "#     # print(f'[Bala] betas\\n{betas.shape}\\n{betas}')\n",
    "\n",
    "#     if not timestep_respacing:\n",
    "#         timestep_respacing = [diffusion_steps]\n",
    "\n",
    "#     # use beta to calculate alpha, and use alpha to update new beta (why?)\n",
    "#     diffusion = SpacedDiffusion(\n",
    "#         use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "#         betas=betas,\n",
    "#         rescale_timesteps=rescale_timesteps,\n",
    "#         predict_xstart=predict_xstart,\n",
    "#         learn_sigmas = learn_sigma,\n",
    "#         sigma_small = sigma_small,\n",
    "#         use_kl = use_kl,\n",
    "#         rescale_learned_sigmas=rescale_learned_sigmas\n",
    "#     )\n",
    "\n",
    "#     return model, diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1595f017",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2c1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_text(\n",
    "    batch_size, \n",
    "    seq_len, \n",
    "    dataset, \n",
    "    data_dir,\n",
    "    deterministic=False,  \n",
    "    model_emb=None,\n",
    "    split='train', \n",
    "    loaded_vocab=None,\n",
    "    loop=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    For a dataset, create a generator over (seqs, kwargs) pairs.\n",
    "\n",
    "    Each seq is an (bsz, len, h) float tensor, and the kwargs dict contains zero or\n",
    "    more keys, each of which map to a batched Tensor of their own.\n",
    "    The kwargs dict can be used for some meta information.\n",
    "\n",
    "    :param batch_size: the batch size of each returned pair.\n",
    "    :param seq_len: the max sequence length (one-side).\n",
    "    :param deterministic: if True, yield results in a deterministic order.\n",
    "    :param data_args: including dataset directory, num of dataset, basic settings, etc.\n",
    "    :param model_emb: loaded word embeddings.\n",
    "    :param loaded_vocab: loaded word vocabs.\n",
    "    :param loop: loop to get batch data or not.\n",
    "    \"\"\"\n",
    "\n",
    "    print('#'*30, '\\nLoading text data...')\n",
    "\n",
    "    # training_data -> DataDict{train: Dataset{feature: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'], num_rows: n}}\n",
    "    training_data = get_corpus(dataset, data_dir, seq_len, split=split, loaded_vocab=loaded_vocab)\n",
    "    \n",
    "    dataset = TextDataset(\n",
    "        training_data,\n",
    "        model_emb=model_emb\n",
    "    )\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,  # 20,\n",
    "        # drop_last=True,\n",
    "        shuffle=not deterministic,\n",
    "        num_workers=0,\n",
    "    )\n",
    "    if loop:\n",
    "        return infinite_loader(data_loader)\n",
    "    else:\n",
    "        # print(data_loader)\n",
    "        return iter(data_loader)\n",
    "\n",
    "def infinite_loader(data_loader):\n",
    "    while True:\n",
    "        yield from data_loader\n",
    "\n",
    "def helper_tokenize(sentence_lst, vocab_dict, seq_len):\n",
    "    # Process.memory_info is expressed in bytes, so convert to megabytes\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "    raw_datasets = Dataset2.from_dict(sentence_lst)\n",
    "    print(raw_datasets)\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        input_id_x = vocab_dict.encode_token(examples['src'])\n",
    "        input_id_y = vocab_dict.encode_token(examples['trg'])\n",
    "        result_dict = {'input_id_x': input_id_x, 'input_id_y': input_id_y}\n",
    "\n",
    "        return result_dict\n",
    "\n",
    "    # Tokenize the data x and y\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=['src', 'trg'],\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "    print('### tokenized_datasets', tokenized_datasets)\n",
    "    print('### tokenized_datasets...example', tokenized_datasets['input_id_x'][0])\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    def merge_and_mask(group_lst):\n",
    "        lst = []\n",
    "        mask = []\n",
    "        for i in range(len(group_lst['input_id_x'])):\n",
    "            end_token = group_lst['input_id_x'][i][-1]\n",
    "            src = group_lst['input_id_x'][i][:-1]\n",
    "            trg = group_lst['input_id_y'][i][:-1]\n",
    "            while len(src) + len(trg) > seq_len - 3:\n",
    "                if len(src)>len(trg):\n",
    "                    src.pop()\n",
    "                elif len(src)<len(trg):\n",
    "                    trg.pop()\n",
    "                else:\n",
    "                    src.pop()\n",
    "                    trg.pop()\n",
    "            src.append(end_token)\n",
    "            trg.append(end_token)\n",
    "            \n",
    "            lst.append(src + [vocab_dict.sep_token_id] + trg)\n",
    "            mask.append([0]*(len(src)+1))\n",
    "        group_lst['input_ids'] = lst\n",
    "        group_lst['input_mask'] = mask\n",
    "        return group_lst\n",
    "    \n",
    "    # Merge data x+[sep]+y\n",
    "    # Mask  data 0(x+[sep])\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        merge_and_mask,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"merge and mask\",\n",
    "    )\n",
    "    \n",
    "    def pad_function(group_lst):\n",
    "        max_length = seq_len\n",
    "        # print(f'[Bala] pad: {vocab_dict.pad_token_id}')\n",
    "        group_lst['input_ids'] = _collate_batch_helper(group_lst['input_ids'], vocab_dict.pad_token_id, max_length)\n",
    "        group_lst['input_mask'] = _collate_batch_helper(group_lst['input_mask'], 1, max_length)\n",
    "        return group_lst\n",
    "\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    # trim data and add padding\n",
    "    lm_datasets = tokenized_datasets.map(\n",
    "        pad_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        desc=f\"padding\",\n",
    "    )\n",
    "\n",
    "    print(lm_datasets, 'padded dataset')\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    raw_datasets = datasets.DatasetDict()\n",
    "    raw_datasets['train'] = lm_datasets\n",
    "    print(f\"RAM used: {psutil.Process().memory_info().rss / (1024 * 1024):.2f} MB\")\n",
    "    return raw_datasets\n",
    "\n",
    "\n",
    "def get_corpus(dataset, data_dir, seq_len, split='train', loaded_vocab=None):\n",
    "\n",
    "    print('#'*30, '\\nLoading dataset {} from {}...'.format(dataset, data_dir))\n",
    "\n",
    "    sentence_lst = {'src':[], 'trg': []}\n",
    "    \n",
    "    if split == 'train':\n",
    "        print('### Loading form the TRAIN set...')\n",
    "        path = f'{data_dir}/train.jsonl'\n",
    "    elif split == 'valid':\n",
    "        print('### Loading form the VALID set...')\n",
    "        path = f'{data_dir}/valid.jsonl'\n",
    "    elif split == 'test':\n",
    "        print('### Loading form the TEST set...')\n",
    "        path = f'{data_dir}/test.jsonl'\n",
    "    elif split == 'test_128_seed666':\n",
    "        print('### Loading form the TEST set...')\n",
    "        path = f'{data_dir}/test_128_seed666.jsonl'\n",
    "    else:\n",
    "        assert False, \"invalid split for dataset\"\n",
    "\n",
    "    with open(path, 'r') as f_reader:\n",
    "        for row in f_reader:\n",
    "            sentence_lst['src'].append(json.loads(row)['src'].strip())\n",
    "            sentence_lst['trg'].append(json.loads(row)['trg'].strip())\n",
    "\n",
    "    print('### Data samples...\\n', sentence_lst['src'][:2], sentence_lst['trg'][:2])\n",
    "        \n",
    "    # get tokenizer.\n",
    "    vocab_dict = loaded_vocab\n",
    "\n",
    "    train_dataset = helper_tokenize(sentence_lst, vocab_dict, seq_len)\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_datasets, model_emb=None):\n",
    "        super().__init__()\n",
    "        self.text_datasets = text_datasets\n",
    "        self.length = len(self.text_datasets['train'])\n",
    "        # self.data_args = data_args\n",
    "        self.model_emb = model_emb\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with th.no_grad():\n",
    "\n",
    "            # input_ids -> tokenized input data\n",
    "            input_ids = self.text_datasets['train'][idx]['input_ids']\n",
    "            # hidden_state -> data after embedding\n",
    "            hidden_state = self.model_emb(th.tensor(input_ids).to(dev()))\n",
    "            \n",
    "            # obtain the input vectors, only used when word embedding is fixed (not trained end-to-end)\n",
    "            arr = np.array(hidden_state.cpu(), dtype=np.float32)\n",
    "\n",
    "            out_kwargs = {}\n",
    "            out_kwargs['input_ids'] = np.array(self.text_datasets['train'][idx]['input_ids'])\n",
    "            out_kwargs['input_mask'] = np.array(self.text_datasets['train'][idx]['input_mask'])\n",
    "            # print(f'[Bala] input_ids: {out_kwargs['input_ids']}')\n",
    "            # print(f'[Bala] input_mask: {out_kwargs['input_mask']}')\n",
    "\n",
    "            return arr, out_kwargs\n",
    "\n",
    "def _collate_batch_helper(examples, pad_token_id, max_length, return_mask=False):\n",
    "    result = th.full([len(examples), max_length], pad_token_id, dtype=th.int64).tolist()\n",
    "    mask_ = th.full([len(examples), max_length], pad_token_id, dtype=th.int64).tolist()\n",
    "    for i, example in enumerate(examples):\n",
    "        curr_len = min(len(example), max_length)\n",
    "        result[i][:curr_len] = example[:curr_len]\n",
    "        mask_[i][:curr_len] = [1] * curr_len\n",
    "    if return_mask:\n",
    "        return result, mask_\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7a6d5",
   "metadata": {},
   "source": [
    "### gaussian_diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d1aa212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion:\n",
    "    \"\"\"\n",
    "    Utilities for training and sampling diffusion models.\n",
    "\n",
    "    Ported directly from here, and then adapted over time to further experimentation.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n",
    "\n",
    "    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n",
    "                  starting at T and going to 1.\n",
    "    :param predict_xstart: the model outputs to predict x_0, else to predict eps.\n",
    "    :param learn_sigmas: the model outputs to predict sigma or not. Default: False\n",
    "    :param rescale_learned_sigmas, sigma_small: details setting of learned sigmas\n",
    "    :param rescale_timesteps: if True, pass floating point timesteps into the\n",
    "                              model so that they are always scaled like in the\n",
    "                              original paper (0 to 1000).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        betas,\n",
    "        predict_xstart,\n",
    "        rescale_learned_sigmas,\n",
    "        learn_sigmas,\n",
    "        sigma_small,\n",
    "        use_kl,\n",
    "        rescale_timesteps=False,\n",
    "    ):\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "        self.predict_xstart = predict_xstart\n",
    "        self.rescale_learned_sigmas = rescale_learned_sigmas\n",
    "        self.learn_sigmas = learn_sigmas\n",
    "        self.sigma_small = sigma_small\n",
    "        self.use_kl = use_kl\n",
    "\n",
    "        # Use float64 for accuracy.\n",
    "        betas = np.array(betas, dtype=np.float64)\n",
    "        self.betas = betas\n",
    "        # assert len(betas.shape) == 1, \"betas must be 1-D\" # we have new batch betas\n",
    "        assert (betas > 0).all() and (betas <= 1).all()\n",
    "\n",
    "        self.num_timesteps = int(betas.shape[1])\n",
    "\n",
    "        # formula from diffusion-LM\n",
    "        alphas = 1.0 - betas\n",
    "        self.alphas_cumprod = np.cumprod(alphas, axis=1)\n",
    "        # self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1])\n",
    "        self.alphas_cumprod_prev = np.insert(self.alphas_cumprod, 0, 1.0, axis=1)[:, :-1]\n",
    "        # self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0) # only use for ddim\n",
    "        # assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)\n",
    "        \n",
    "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
    "        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)\n",
    "        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)\n",
    "\n",
    "        # calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        \n",
    "        # log calculation clipped because the posterior variance is 0 at the\n",
    "        # beginning of the diffusion chain.\n",
    "        # self.posterior_log_variance_clipped = np.log(\n",
    "        #     np.append(self.posterior_variance[1], self.posterior_variance[1:])\n",
    "        # )\n",
    "        self.posterior_log_variance_clipped = np.log(\n",
    "            np.insert(self.posterior_variance, 1, self.posterior_variance[:, 1], axis=1)[:, 1:]\n",
    "        )\n",
    "        self.posterior_mean_coef1 = (\n",
    "            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        self.posterior_mean_coef2 = (\n",
    "            (1.0 - self.alphas_cumprod_prev)\n",
    "            * np.sqrt(alphas)\n",
    "            / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "        # Bala edit: alpha shape -> (batch, diffusion_steps)\n",
    "        \n",
    "        self.mapping_func = None # implement in train main()\n",
    "        self.add_mask_noise = False # TODO\n",
    "        \n",
    "\n",
    "    def training_losses(self, model, *args, **kwargs):\n",
    "        self.model = model\n",
    "        return self.training_losses_seq2seq(model, *args, **kwargs)\n",
    "\n",
    "    def _predict_xstart_from_eps(self, x_t, t, offsets, eps):\n",
    "        assert x_t.shape == eps.shape\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape, offsets) * x_t\n",
    "            - _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape, offsets) * eps\n",
    "        )\n",
    "\n",
    "    def _predict_eps_from_xstart(self, x_t, t, pred_xstart, offsets):\n",
    "        return (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape, offsets) * x_t\n",
    "            - pred_xstart\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape, offsets)\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        if self.rescale_timesteps:\n",
    "            return t.float() * (1000.0 / self.num_timesteps)\n",
    "        return t\n",
    "\n",
    "    def q_mean_variance(self, x_start, t, offsets):\n",
    "        \"\"\"\n",
    "        Get the distribution q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n",
    "        \"\"\"\n",
    "        mean = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape, offsets) * x_start\n",
    "        )\n",
    "        variance = _extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape, offsets)\n",
    "        log_variance = _extract_into_tensor(\n",
    "            self.log_one_minus_alphas_cumprod, t, x_start.shape, offsets\n",
    "        )\n",
    "        return mean, variance, log_variance\n",
    "\n",
    "    def q_sample(self, x_start, t, offsets, noise=None, mask=None):\n",
    "        \"\"\"\n",
    "        Diffuse the data for a given number of diffusion steps.\n",
    "\n",
    "        In other words, sample from q(x_t | x_0).\n",
    "\n",
    "        :param x_start: the initial data batch.\n",
    "        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n",
    "        :param noise: if specified, the split-out normal noise.\n",
    "        :param mask: anchoring masked position\n",
    "        :return: A noisy version of x_start.\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "\n",
    "        assert noise.shape == x_start.shape\n",
    "        \n",
    "        # print('t', t.shape)\n",
    "        # print('x_start', x_start.shape)\n",
    "        # print('sqrt_alphas_cumprod', self.sqrt_alphas_cumprod.shape)\n",
    "        # print('after', _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape, offsets).shape)\n",
    "        \n",
    "        x_t = (\n",
    "            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape, offsets) * x_start\n",
    "            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape, offsets)\n",
    "            * noise\n",
    "        )\n",
    "\n",
    "        if mask == None:\n",
    "            return x_t\n",
    "        else:\n",
    "            # expand mask shape from (batch, seqlen) to (batch, sqelen, emb_dim)\n",
    "            # mask.unsqueeze(dim=-1) -> from (batch, seqlen) to (batch, seqlen, 1)\n",
    "            mask = th.broadcast_to(mask.unsqueeze(dim=-1), x_start.shape)\n",
    "            return th.where(mask==0, x_start, x_t)\n",
    "\n",
    "    def q_posterior_mean_variance(self, x_start, x_t, t, offsets):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the diffusion posterior: \n",
    "            q(x_{t-1} | x_t, x_0)\n",
    "\n",
    "        \"\"\"\n",
    "        assert x_start.shape == x_t.shape\n",
    "        posterior_mean = (\n",
    "            _extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape, offsets) * x_start\n",
    "            + _extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape, offsets) * x_t\n",
    "        )\n",
    "        posterior_variance = _extract_into_tensor(self.posterior_variance, t, x_t.shape, offsets)\n",
    "        posterior_log_variance_clipped = _extract_into_tensor(\n",
    "            self.posterior_log_variance_clipped, t, x_t.shape, offsets\n",
    "        )\n",
    "        assert (\n",
    "            posterior_mean.shape[0]\n",
    "            == posterior_variance.shape[0]\n",
    "            == posterior_log_variance_clipped.shape[0]\n",
    "            == x_start.shape[0]\n",
    "        )\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    # bala edit this function\n",
    "    def p_mean_variance(\n",
    "        self, model, x, t, offsets, clip_denoised=True, denoised_fn=None, model_kwargs=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Apply the model to get p(x_{t-1} | x_t), as well as a prediction of\n",
    "        the initial x, x_0.\n",
    "\n",
    "        :param model: the model, which takes a signal and a batch of timesteps\n",
    "                      as input.\n",
    "        :param x: the [N x C x ...] tensor at time t.\n",
    "        :param t: a 1-D Tensor of timesteps.\n",
    "        :param clip_denoised: if True, clip the denoised signal into [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample. Applies before\n",
    "            clip_denoised.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict with the following keys:\n",
    "                 - 'mean': the model mean output.\n",
    "                 - 'variance': the model variance output.\n",
    "                 - 'log_variance': the log of 'variance'.\n",
    "                 - 'pred_xstart': the prediction for x_0.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "\n",
    "        B, C = x.size(0), x.size(-1)\n",
    "        assert t.shape == (B,)\n",
    "        model_output = model(x, self._scale_timesteps(t), **model_kwargs)\n",
    "        # print('model_output', sum(model_output[0]))\n",
    "        \n",
    "        # for fixedlarge, we set the initial (log-)variance like so\n",
    "        # to get a better decoder log likelihood.\n",
    "        # model_variance = np.append(self.posterior_variance[1], self.betas[1:])\n",
    "        model_variance = np.insert(self.betas, 1, self.posterior_variance[:, 1], axis=1)[:, 1:]\n",
    "        # print(f'[apple] model_variance: {model_variance.shape}\\n{model_variance}') # -> (diffusion_steps, )\n",
    "        model_log_variance = np.log(model_variance)\n",
    "        # print(f'[apple] model_log_variance: {model_log_variance.shape}')\n",
    "        \n",
    "        model_variance = _extract_into_tensor(model_variance, t, x.shape, offsets)\n",
    "        model_log_variance = _extract_into_tensor(model_log_variance, t, x.shape, offsets)\n",
    "\n",
    "        def process_xstart(x):\n",
    "            if denoised_fn is not None:\n",
    "                # print(denoised_fn)\n",
    "                x = denoised_fn(x, t)\n",
    "            if clip_denoised:\n",
    "                return x.clamp(-1, 1)\n",
    "            return x\n",
    "\n",
    "        if self.predict_xstart:\n",
    "            # knn rounding -> using most close token as input, and get new emb output\n",
    "            pred_xstart = process_xstart(model_output)\n",
    "        else:\n",
    "            ### model is used to predict eps\n",
    "            pred_xstart = process_xstart(\n",
    "                self._predict_xstart_from_eps(x_t=x, t=t, offsets=offsets, eps=model_output)\n",
    "            )\n",
    "\n",
    "        # μ_t\n",
    "        model_mean, _, _ = self.q_posterior_mean_variance(\n",
    "            x_start=pred_xstart, x_t=x, t=t, offsets=offsets\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            model_mean.shape == model_log_variance.shape == pred_xstart.shape == x.shape\n",
    "        )\n",
    "        return {\n",
    "            \"mean\": model_mean,\n",
    "            \"variance\": model_variance,\n",
    "            \"log_variance\": model_log_variance,\n",
    "            \"pred_xstart\": pred_xstart,\n",
    "        }\n",
    "\n",
    "    def p_sample(\n",
    "        self, model, x, t, offsets, clip_denoised=True, denoised_fn=None, model_kwargs=None,\n",
    "            top_p=None, mask=None, x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model at the given timestep.\n",
    "\n",
    "        :param model: the model to sample from.\n",
    "        :param x: the current tensor at x_{t-1}.\n",
    "        :param t: the value of t, starting at 0 for the first diffusion step.\n",
    "        :param clip_denoised: if True, clip the x_start prediction to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :return: a dict containing the following keys:\n",
    "                 - 'sample': a random sample from the model.\n",
    "                 - 'pred_xstart': a prediction of x_0.\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            offsets,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        if top_p is not None and top_p > 0:\n",
    "            # print('top_p sampling')\n",
    "            noise = th.randn_like(x)\n",
    "            replace_mask = th.abs(noise) > top_p\n",
    "            while replace_mask.any():\n",
    "                noise[replace_mask] = th.randn_like(noise[replace_mask])\n",
    "                replace_mask = th.abs(noise) > top_p\n",
    "            assert (th.abs(noise) <= top_p).all()\n",
    "\n",
    "        else:\n",
    "            noise = th.randn_like(x)\n",
    "\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        # get new sample from mean and variance \n",
    "        sample = out[\"mean\"] + nonzero_mask * th.exp(0.5 * out[\"log_variance\"]) * noise\n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            # only denoise y, x keep use x0\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "\n",
    "        return {\n",
    "            \"sample\": sample, \n",
    "            \"pred_xstart\": out[\"pred_xstart\"], # after knn rounding model output\n",
    "            \"greedy_mean\": out[\"mean\"], \n",
    "            \"out\": out\n",
    "        }\n",
    "\n",
    "    \n",
    "    def p_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        offsets,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model.\n",
    "\n",
    "        :param model: the model module.\n",
    "        :param shape: the shape of the samples, (N, C, H, W).\n",
    "        :param noise: if specified, the noise from the encoder to sample.\n",
    "                      Should be of the same shape as `shape`.\n",
    "        :param clip_denoised: if True, clip x_start predictions to [-1, 1].\n",
    "        :param denoised_fn: if not None, a function which applies to the\n",
    "            x_start prediction before it is used to sample.\n",
    "        :param mask: anchoring masked position to x_start\n",
    "        :param clamp_step: in clamp_first mode, choose end clamp step, otherwise starting clamp step\n",
    "        :param clamp_first: bool, clamp_first mode\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param device: if specified, the device to create the samples on.\n",
    "                       If not specified, use a model parameter's device.\n",
    "        :param progress: if True, show a tqdm progress bar.\n",
    "        :return: a non-differentiable batch of samples.\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.p_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            offsets,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress, # False\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=clamp_first,\n",
    "            mask=mask,\n",
    "            x_start=x_start\n",
    "        ):\n",
    "            final.append(sample['sample'].tolist())\n",
    "        return final\n",
    "\n",
    "    def p_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        offsets,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model and yield intermediate samples from\n",
    "        each timestep of diffusion.\n",
    "\n",
    "        Arguments are the same as p_sample_loop().\n",
    "        Returns a generator over dicts, where each dict is the return value of\n",
    "        p_sample().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None: # custom your the start point of x_0\n",
    "            sample_x = noise # mask x\n",
    "        else:\n",
    "            # shape = (data len, seq len, emb dim)\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        # [1999, 1998, ..., 1, 0]\n",
    "        indices = list(range(self.num_timesteps))[::-1]\n",
    "\n",
    "        if progress: # not in \n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices: # from T to 0\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            if not clamp_first:\n",
    "                if i > clamp_step:\n",
    "                    denoised_fn_cur = None\n",
    "                else:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "            else:\n",
    "                if i >= clamp_step:\n",
    "                    denoised_fn_cur = denoised_fn\n",
    "                else:\n",
    "                    denoised_fn_cur = None\n",
    "            with th.no_grad():\n",
    "                out = self.p_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    offsets,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn_cur,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    top_p=top_p,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                if i == 0:\n",
    "                    yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "\n",
    "    def _get_x_start(self, x_start_mean, std):\n",
    "        '''\n",
    "        Word embedding projection from {Emb(w)} to {x_0}\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        noise = th.randn_like(x_start_mean)\n",
    "        assert noise.shape == x_start_mean.shape\n",
    "        return (\n",
    "             x_start_mean + std * noise\n",
    "        )\n",
    "\n",
    "    def _token_discrete_loss(self, x_t, get_logits, input_ids, mask=None, truncate=False, t=None):\n",
    "        '''\n",
    "        the loss of -log p(w|z_0)\n",
    "        :param x_start_mean: word embedding\n",
    "        :return: x_0\n",
    "        '''\n",
    "        reshaped_x_t = x_t\n",
    "        logits = get_logits(reshaped_x_t)  # bsz, seqlen, vocab\n",
    "        # print(logits.shape)\n",
    "        loss_fct = th.nn.CrossEntropyLoss(reduction='none') # contain softmax and log (compare with nn.NLLLoss)\n",
    "        # logits.view(-1, logits.size(-1)) -> (bsz*seqlen, vocab) / input_ids.view(-1) -> (bsz*seqlen)\n",
    "        # decoder_nll -> (bsz, seqlen)\n",
    "        decoder_nll = loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape)\n",
    "        # print(f'[Bala] {loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).shape}')\n",
    "        # print(f'[Bala] view: {loss_fct(logits.view(-1, logits.size(-1)), input_ids.view(-1)).view(input_ids.shape).shape}')\n",
    "        if mask != None:\n",
    "            decoder_nll *= mask # only y\n",
    "        # print(decoder_nll.shape)\n",
    "        if mask != None:\n",
    "            decoder_nll = decoder_nll.sum(dim=-1)/mask.sum(dim=-1) # each y mean nll loss\n",
    "        else:\n",
    "            decoder_nll = decoder_nll.mean(dim=-1) # each sentence mean nll loss\n",
    "\n",
    "        return decoder_nll\n",
    "\n",
    "    def _x0_helper(self, model_output, x, t, offsets):\n",
    "\n",
    "        if self.predict_xstart: # true\n",
    "            pred_xstart = model_output\n",
    "            # pred_prev -> q(x_{t-1} | x_t, x_0) mean\n",
    "            pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t, offsets=offsets\n",
    "            )\n",
    "\n",
    "        else: # predict eps\n",
    "            pred_xstart = self._predict_xstart_from_eps(x_t=x, t=t, offsets=offsets, eps=model_output)\n",
    "        \n",
    "            pred_prev, _, _ = self.q_posterior_mean_variance(\n",
    "                x_start=pred_xstart, x_t=x, t=t, offsets=offsets\n",
    "            )\n",
    "\n",
    "        return {'pred_xprev':pred_prev, 'pred_xstart':pred_xstart}\n",
    "\n",
    "    # Bala edit this function\n",
    "    def training_losses_seq2seq(self, model, x_start, t, offsets, ids, mask, noise=None):\n",
    "        \"\"\"\n",
    "        Compute training losses for a single timestep.\n",
    "\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs. # not used unless fixing the input embeddings\n",
    "        :param t: a batch of timestep indices.\n",
    "        :param ids: from origial model_kwargs dict\n",
    "        :param mask: from origial model_kwargs dict\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param noise: if specified, the specific Gaussian noise to try to remove.\n",
    "        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                 Some mean or variance settings may also have other keys.\n",
    "        \"\"\"\n",
    "        x_start_fix = x_start # save the orignal x_0 \n",
    "        # assert 'input_ids' in model_kwargs\n",
    "        # input_ids_x = model_kwargs.pop('input_ids').to(t.device) # x + [sep] + y\n",
    "        # input_ids_mask = model_kwargs.pop('input_mask').to(t.device) # y\n",
    "        # print(f'[Bala] input_ids_x: {input_ids_x.shape}')\n",
    "        input_ids_x = ids\n",
    "        input_ids_mask = mask\n",
    "        \n",
    "        # just a word embedding\n",
    "        x_start_mean = model.model.module.get_embeds(input_ids_x)\n",
    "        # x_start_mean = model.module.get_embeds(input_ids_x)\n",
    "        \n",
    "        # print(f'[Bala] self.sqrt_one_minus_alphas_cumprod: {self.sqrt_one_minus_alphas_cumprod}')\n",
    "        std = _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod,\n",
    "                                   th.tensor([0]*input_ids_x.shape[0]).to(x_start_mean.device),\n",
    "                                   x_start_mean.shape,\n",
    "                                   offsets)\n",
    "\n",
    "        # x_start_log_var = 2 * th.log(std)\n",
    "        x_start = self._get_x_start(x_start_mean, std) # {Emb(w)} to {x_0}\n",
    "        if noise is None:\n",
    "            noise = th.randn_like(x_start)\n",
    "\n",
    "        # a nosie version of x_start, only impose noise on y\n",
    "        x_t = self.q_sample(x_start, t, offsets, noise=noise, mask=input_ids_mask) # reparametrization trick.\n",
    "        # print(f'[Bala] mask: {input_ids_mask}')\n",
    "        # print(f'[Bala] x_start: {x_start.shape}')\n",
    "        # print(f'[Bala] x_t: {x_t.shape}') # -> (batch, sqelen, emb_dim)\n",
    "        \n",
    "        del x_start_fix, std, noise\n",
    "        gc.collect\n",
    "        \n",
    "        # nn.Linear(input_dim, vocabulary_size)\n",
    "        get_logits = model.model.module.get_logits\n",
    "        # get_logits = model.module.get_logits\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        target = x_start\n",
    "        # print(f'target: {target.shape}')\n",
    "        # model_input = x_t.view(1, x_t.shape[0], x_t.shape[1]).to(dev())\n",
    "        # print(f'model_input: {model_input.shape}')\n",
    "        # print(f'self._scale_timesteps(t): {self._scale_timesteps(t)}')\n",
    "        model_output = model(x_t, self._scale_timesteps(t))\n",
    "        # print(f'[Bala] model_output: {model_output.shape}')\n",
    "        assert model_output.shape == target.shape == x_start.shape\n",
    "        terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "        \n",
    "        model_out_x_start = self._x0_helper(model_output, x_t, t, offsets)['pred_xstart'] # predicted_xstart = model_output\n",
    "        # print(f'[Bala] t: {t}')\n",
    "        t0_mask = (t == 0)\n",
    "        # print(f'[Bala] t0_mask: {t0_mask}')\n",
    "        t0_loss = mean_flat((x_start_mean - model_out_x_start) ** 2)\n",
    "        terms[\"mse\"] = th.where(t0_mask, t0_loss, terms[\"mse\"])\n",
    "        \n",
    "        del x_start_mean, x_t, t0_loss, model_output, target\n",
    "        gc.collect()\n",
    "\n",
    "        # tT_mask = (t == self.num_timesteps - 1)\n",
    "        out_mean, _, _ = self.q_mean_variance(x_start, th.LongTensor([self.num_timesteps - 1]*input_ids_x.shape[0]).to(x_start.device), offsets)\n",
    "        # print(f'[Bala] out_mean: {out_mean}\\n{out_mean.shape}')\n",
    "        tT_loss =  mean_flat(out_mean ** 2)\n",
    "        \n",
    "        # each sentence nll loss\n",
    "        decoder_nll = self._token_discrete_loss(x_start, get_logits, input_ids_x) # embedding regularization\n",
    "        # only y nll loss\n",
    "        terms[\"nll\"] = self._token_discrete_loss(model_out_x_start, get_logits, input_ids_x, mask=input_ids_mask, truncate=True, t=t) # x_0->model_out_x_start\n",
    "        # assert (model.lm_head.weight == model.word_embedding.weight).all()\n",
    "        \n",
    "        terms[\"loss\"] = terms[\"mse\"] + decoder_nll + tT_loss\n",
    "        del  x_start, input_ids_x, input_ids_mask, model_out_x_start, out_mean, tT_loss, decoder_nll, get_logits\n",
    "        gc.collect()\n",
    "        # th.cuda.empty_cache()\n",
    "\n",
    "        return terms\n",
    "\n",
    "    def ddim_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        offsets,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t-1} from the model using DDIM.\n",
    "\n",
    "        Same usage as p_sample().\n",
    "        \"\"\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            offsets,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = self._predict_eps_from_xstart(x, t, out[\"pred_xstart\"], offsets)\n",
    "        alpha_bar = _extract_into_tensor(self.alphas_cumprod, t, x.shape, offsets)\n",
    "        alpha_bar_prev = _extract_into_tensor(self.alphas_cumprod_prev, t, x.shape, offsets)\n",
    "        sigma = (\n",
    "            eta\n",
    "            * th.sqrt((1 - alpha_bar_prev) / (1 - alpha_bar))\n",
    "            * th.sqrt(1 - alpha_bar / alpha_bar_prev)\n",
    "        )\n",
    "        # Equation 12.\n",
    "        noise = th.randn_like(x)\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_prev)\n",
    "            + th.sqrt(1 - alpha_bar_prev - sigma ** 2) * eps\n",
    "        )\n",
    "        nonzero_mask = (\n",
    "            (t != 0).float().view(-1, *([1] * (len(x.shape) - 1)))\n",
    "        )  # no noise when t == 0\n",
    "        # print(sigma.mean())\n",
    "        sample = mean_pred + nonzero_mask * sigma * noise\n",
    "        if langevin_fn:\n",
    "            print(t.shape)\n",
    "            sample=langevin_fn(sample, mean_pred, sigma, self.alphas_cumprod_prev[t[0]], t, x)\n",
    "        \n",
    "        if mask == None:\n",
    "            pass\n",
    "        else:\n",
    "            sample = th.where(mask==0, x_start, sample)\n",
    "        \n",
    "        return {\"sample\": sample, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_reverse_sample(\n",
    "        self,\n",
    "        model,\n",
    "        x,\n",
    "        t,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        eta=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sample x_{t+1} from the model using DDIM reverse ODE.\n",
    "        \"\"\"\n",
    "        assert eta == 0.0, \"Reverse ODE only for deterministic path\"\n",
    "        out = self.p_mean_variance(\n",
    "            model,\n",
    "            x,\n",
    "            t,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "        )\n",
    "        # Usually our model outputs epsilon, but we re-derive it\n",
    "        # in case we used x_start or x_prev prediction.\n",
    "        eps = (\n",
    "            _extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x.shape) * x\n",
    "            - out[\"pred_xstart\"]\n",
    "        ) / _extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x.shape)\n",
    "        alpha_bar_next = _extract_into_tensor(self.alphas_cumprod_next, t, x.shape)\n",
    "\n",
    "        # Equation 12. reversed\n",
    "        mean_pred = (\n",
    "            out[\"pred_xstart\"] * th.sqrt(alpha_bar_next)\n",
    "            + th.sqrt(1 - alpha_bar_next) * eps\n",
    "        )\n",
    "\n",
    "        return {\"sample\": mean_pred, \"pred_xstart\": out[\"pred_xstart\"]}\n",
    "\n",
    "    def ddim_sample_loop(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        offsets,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        top_p=None,\n",
    "        clamp_step=None,\n",
    "        clamp_first=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "        beta_ins = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate samples from the model using DDIM.\n",
    "        :param gap: compute ddim sampling for each {gap} step\n",
    "\n",
    "        Same usage as p_sample_loop().\n",
    "        \"\"\"\n",
    "        final = []\n",
    "        for sample in self.ddim_sample_loop_progressive(\n",
    "            model,\n",
    "            shape,\n",
    "            offsets,\n",
    "            noise=noise,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=denoised_fn,\n",
    "            model_kwargs=model_kwargs,\n",
    "            device=device,\n",
    "            progress=progress,\n",
    "            mask=mask,\n",
    "            x_start=x_start,\n",
    "            gap = gap,\n",
    "            beta_ins = beta_ins,\n",
    "        ):\n",
    "            final.append(sample['sample'].tolist())\n",
    "        return final\n",
    "\n",
    "    def ddim_sample_loop_progressive(\n",
    "        self,\n",
    "        model,\n",
    "        shape,\n",
    "        offsets,\n",
    "        noise=None,\n",
    "        clip_denoised=True,\n",
    "        denoised_fn=None,\n",
    "        model_kwargs=None,\n",
    "        device=None,\n",
    "        progress=False,\n",
    "        eta=0.0,\n",
    "        langevin_fn=None,\n",
    "        mask=None,\n",
    "        x_start=None,\n",
    "        gap=1,\n",
    "        beta_ins=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Use DDIM to sample from the model and yield intermediate samples from\n",
    "        each timestep of DDIM.\n",
    "\n",
    "        Same usage as p_sample_loop_progressive().\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = next(model.parameters()).device\n",
    "        assert isinstance(shape, (tuple, list))\n",
    "        if noise is not None:\n",
    "            sample_x = noise\n",
    "        else:\n",
    "            sample_x = th.randn(*shape, device=device)\n",
    "        # indices = list(range(self.num_timesteps))[::-1][::gap]\n",
    "        indices = [i for i, e in enumerate(beta_ins) if e ==1]\n",
    "\n",
    "        if progress:\n",
    "            # Lazy import so that we don't depend on tqdm.\n",
    "            from tqdm.auto import tqdm\n",
    "\n",
    "            indices = tqdm(indices)\n",
    "\n",
    "        for i in indices:\n",
    "            t = th.tensor([i] * shape[0], device=device)\n",
    "            with th.no_grad():\n",
    "                out = self.ddim_sample(\n",
    "                    model,\n",
    "                    sample_x,\n",
    "                    t,\n",
    "                    offsets,\n",
    "                    clip_denoised=clip_denoised,\n",
    "                    denoised_fn=denoised_fn,\n",
    "                    model_kwargs=model_kwargs,\n",
    "                    mask=mask,\n",
    "                    x_start=x_start\n",
    "                )\n",
    "                yield out\n",
    "                sample_x = out[\"sample\"]\n",
    "\n",
    "def _extract_into_tensor(arr, timesteps, broadcast_shape, offsets):\n",
    "    \"\"\"\n",
    "    Extract values from a 1-D numpy array for a batch of indices.\n",
    "\n",
    "    :param arr: the 1-D numpy array.\n",
    "    :param timesteps: a tensor of indices into the array to extract.\n",
    "    :param broadcast_shape: a larger shape of K dimensions with the batch\n",
    "                            dimension equal to the length of timesteps.\n",
    "    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\n",
    "    \"\"\"\n",
    "    # Bala edit\n",
    "    # print(f'[Bala] timesteps: {timesteps.shape}, {timesteps}')\n",
    "    # res = th.from_numpy(arr).to(device=timesteps.device)[:,timesteps].float()\n",
    "    start_index = offsets*timesteps.shape[0]\n",
    "    res = th.tensor([arr[start_index+i][timesteps[i]] for i in range(timesteps.shape[0])]).float().to(device=timesteps.device)\n",
    "    # print(f'[Bala] res.shape: {res.shape}')\n",
    "    # print(f'[Bala] broadcast_shape: {broadcast_shape}')\n",
    "    # print(f'[Bala] res b: {res}')\n",
    "    while len(res.shape) < len(broadcast_shape):\n",
    "        res = res[..., None]\n",
    "        # print(f'[Bala] res a: {res}]')\n",
    "    return res.expand(broadcast_shape)\n",
    "\n",
    "\n",
    "def space_timesteps(num_timesteps, section_counts):\n",
    "    \"\"\"\n",
    "    Create a list of timesteps to use from an original diffusion process,\n",
    "    given the number of timesteps we want to take from equally-sized portions\n",
    "    of the original process.\n",
    "\n",
    "    For example, if there's 300 timesteps and the section counts are [10,15,20]\n",
    "    then the first 100 timesteps are strided to be 10 timesteps, the second 100\n",
    "    are strided to be 15 timesteps, and the final 100 are strided to be 20.\n",
    "\n",
    "    If the stride is a string starting with \"ddim\", then the fixed striding\n",
    "    from the DDIM paper is used, and only one section is allowed.\n",
    "\n",
    "    :param num_timesteps: the number of diffusion steps in the original\n",
    "                          process to divide up.\n",
    "    :param section_counts: either a list of numbers, or a string containing\n",
    "                           comma-separated numbers, indicating the step count\n",
    "                           per section. As a special case, use \"ddimN\" where N\n",
    "                           is a number of steps to use the striding from the\n",
    "                           DDIM paper.\n",
    "    :return: a set of diffusion steps from the original process to use.\n",
    "    \"\"\"\n",
    "    # print(f'[Bala] section_counts: {section_counts}')\n",
    "    if isinstance(section_counts, str):\n",
    "        if section_counts.startswith(\"ddim\"):\n",
    "            desired_count = int(section_counts[len(\"ddim\") :])\n",
    "            for i in range(1, num_timesteps):\n",
    "                if len(range(0, num_timesteps, i)) == desired_count:\n",
    "                    return set(range(0, num_timesteps, i))\n",
    "            raise ValueError(\n",
    "                f\"cannot create exactly {num_timesteps} steps with an integer stride\"\n",
    "            )\n",
    "        section_counts = [int(x) for x in section_counts.split(\",\")]\n",
    "    size_per = num_timesteps // len(section_counts)\n",
    "    extra = num_timesteps % len(section_counts)\n",
    "    # print(f'[Bala] size_per: {size_per}')\n",
    "    # print(f'[Bala] extra: {extra}')\n",
    "    start_idx = 0\n",
    "    all_steps = []\n",
    "    for i, section_count in enumerate(section_counts):\n",
    "        # print(f'[Bala] section_count: {section_count}')\n",
    "        size = size_per + (1 if i < extra else 0)\n",
    "        # print(f'[Bala] size: {size}')\n",
    "        if size < section_count:\n",
    "            raise ValueError(\n",
    "                f\"cannot divide section of {size} steps into {section_count}\"\n",
    "            )\n",
    "        if section_count <= 1:\n",
    "            frac_stride = 1\n",
    "        else:\n",
    "            frac_stride = (size - 1) / (section_count - 1)\n",
    "        cur_idx = 0.0\n",
    "        taken_steps = []\n",
    "        for _ in range(section_count):\n",
    "            taken_steps.append(start_idx + round(cur_idx))\n",
    "            cur_idx += frac_stride\n",
    "        all_steps += taken_steps\n",
    "        start_idx += size\n",
    "        # print(f'[Bala] all_steps: {all_steps}')\n",
    "        # print(f'[Bala] start_idx: {start_idx}')\n",
    "    return set(all_steps)\n",
    "\n",
    "\n",
    "class SpacedDiffusion(GaussianDiffusion):\n",
    "    \"\"\"\n",
    "    A diffusion process which can skip steps in a base diffusion process.\n",
    "\n",
    "    :param use_timesteps: a collection (sequence or set) of timesteps from the\n",
    "                          original diffusion process to retain.\n",
    "    :param kwargs: the kwargs to create the base diffusion process.\n",
    "    \"\"\"\n",
    "\n",
    "    # Bala edit\n",
    "    def __init__(self, use_timesteps, **kwargs):\n",
    "        self.use_timesteps = set(use_timesteps)\n",
    "        self.timestep_map = []\n",
    "        self.original_num_steps = kwargs[\"betas\"].shape[1]\n",
    "\n",
    "        # print(kwargs.keys())\n",
    "        base_diffusion = GaussianDiffusion(**kwargs)  # pylint: disable=missing-kwoa\n",
    "        last_alpha_cumprod = 1.0\n",
    "        new_betas = []\n",
    "        # for i, alpha_cumprod in enumerate(base_diffusion.alphas_cumprod):\n",
    "            # if i in self.use_timesteps:\n",
    "                # new_betas.append(1 - alpha_cumprod / last_alpha_cumprod)\n",
    "                # last_alpha_cumprod = alpha_cumprod\n",
    "                # self.timestep_map.append(i)\n",
    "        for i in range(kwargs['betas'].shape[1]):\n",
    "            if i in self.use_timesteps:\n",
    "                new_betas.append(1 - base_diffusion.alphas_cumprod[:, i] / last_alpha_cumprod)\n",
    "                last_alpha_cumprod = base_diffusion.alphas_cumprod[:, i]\n",
    "                self.timestep_map.append(i)\n",
    "        kwargs[\"betas\"] = np.transpose(np.array(new_betas))\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def p_mean_variance(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        # print('called p_mean_var')\n",
    "        return super().p_mean_variance(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def training_losses(\n",
    "        self, model, *args, **kwargs\n",
    "    ):  # pylint: disable=signature-differs\n",
    "        # print('called training_losses')\n",
    "        return super().training_losses(self._wrap_model(model), *args, **kwargs)\n",
    "\n",
    "    def _wrap_model(self, model):\n",
    "        if isinstance(model, _WrappedModel):\n",
    "            return model\n",
    "        return _WrappedModel(\n",
    "            model, self.timestep_map, self.rescale_timesteps, self.original_num_steps\n",
    "        )\n",
    "\n",
    "    def _scale_timesteps(self, t):\n",
    "        # Scaling is done by the wrapped model.\n",
    "        return t\n",
    "    \n",
    "    def get_sqrt_alphas_cumprod(self):\n",
    "        return super().get_sqrt_alphas_cumprod()\n",
    "\n",
    "\n",
    "class _WrappedModel:\n",
    "    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps):\n",
    "        self.model = model\n",
    "        self.timestep_map = timestep_map\n",
    "        self.rescale_timesteps = rescale_timesteps\n",
    "        self.original_num_steps = original_num_steps\n",
    "\n",
    "    def __call__(self, x, ts, **kwargs):\n",
    "        # print(ts)\n",
    "        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n",
    "        new_ts = map_tensor[ts]\n",
    "        # print(new_ts)\n",
    "        if self.rescale_timesteps:\n",
    "            new_ts = new_ts.float() * (1000.0 / self.original_num_steps)\n",
    "        # temp = self.model(x, new_ts, **kwargs)\n",
    "        # print(temp.shape)\n",
    "        # return temp\n",
    "        # print(new_ts)\n",
    "        return self.model(x, new_ts, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7db149",
   "metadata": {},
   "source": [
    "### transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaac9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The full Transformer model with attention and timestep embedding.\n",
    "\n",
    "    :param input_dims: dims of the input Tensor.\n",
    "    :param output_dims: dims of the output Tensor.\n",
    "    :param hidden_t_dim: dims of time embedding.\n",
    "    :param dropout: the dropout probability.\n",
    "    :param config/config_name: the config of PLMs.\n",
    "    :param init_pretrained: bool, init whole network params with PLMs.\n",
    "    :param vocab_size: the size of vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        output_dims,\n",
    "        hidden_t_dim,\n",
    "        dropout=0,\n",
    "        config=None,\n",
    "        config_name='bert-base-uncased',\n",
    "        vocab_size=None,\n",
    "        init_pretrained='no',\n",
    "        logits_mode=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if config is None:\n",
    "            # config -> model's information\n",
    "            # print(f'[Bala] config name: {config_name}')\n",
    "            config = AutoConfig.from_pretrained(config_name)\n",
    "            # print(f'[Bala] config\\n{config}')\n",
    "            config.hidden_dropout_prob = dropout\n",
    "\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_t_dim = hidden_t_dim\n",
    "        self.output_dims = output_dims\n",
    "        self.dropout = dropout\n",
    "        self.logits_mode = logits_mode\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.word_embedding = nn.Embedding(vocab_size, self.input_dims)\n",
    "        self.lm_head = nn.Linear(self.input_dims, vocab_size)\n",
    "        with th.no_grad():\n",
    "            self.lm_head.weight = self.word_embedding.weight\n",
    "\n",
    "        # Feed Forward(?\n",
    "        time_embed_dim = hidden_t_dim * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            linear(hidden_t_dim, time_embed_dim),\n",
    "            SiLU(),\n",
    "            linear(time_embed_dim, config.hidden_size),\n",
    "        )\n",
    "        \n",
    "        # ?\n",
    "        if self.input_dims != config.hidden_size:\n",
    "            self.input_up_proj = nn.Sequential(nn.Linear(input_dims, config.hidden_size),\n",
    "                                              nn.Tanh(), nn.Linear(config.hidden_size, config.hidden_size))\n",
    "        \n",
    "        if init_pretrained == 'bert':\n",
    "            print('initializing from pretrained bert...')\n",
    "            print(config)\n",
    "            temp_bert = BertModel.from_pretrained(config_name, config=config)\n",
    "\n",
    "            self.word_embedding = temp_bert.embeddings.word_embeddings\n",
    "            with th.no_grad():\n",
    "                self.lm_head.weight = self.word_embedding.weight\n",
    "            # self.lm_head.weight.requires_grad = False\n",
    "            # self.word_embedding.weight.requires_grad = False\n",
    "            \n",
    "            self.input_transformers = temp_bert.encoder\n",
    "            self.register_buffer(\"position_ids\", th.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "            self.position_embeddings = temp_bert.embeddings.position_embeddings\n",
    "            self.LayerNorm = temp_bert.embeddings.LayerNorm\n",
    "\n",
    "            del temp_bert.embeddings\n",
    "            del temp_bert.pooler\n",
    "\n",
    "        elif init_pretrained == 'no':\n",
    "            self.input_transformers = BertEncoder(config)\n",
    "\n",
    "            self.register_buffer(\"position_ids\", th.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "            self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "            self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        \n",
    "        else:\n",
    "            assert False, \"invalid type of init_pretrained\"\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        if self.output_dims != config.hidden_size:\n",
    "            self.output_down_proj = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
    "                                                nn.Tanh(), nn.Linear(config.hidden_size, self.output_dims))\n",
    "\n",
    "    def get_embeds(self, input_ids):\n",
    "        return self.word_embedding(input_ids)\n",
    "\n",
    "    def get_logits(self, hidden_repr):\n",
    "        if self.logits_mode == 1:\n",
    "            return self.lm_head(hidden_repr)\n",
    "        elif self.logits_mode == 2: # standard cosine similarity\n",
    "            text_emb = hidden_repr\n",
    "            emb_norm = (self.lm_head.weight ** 2).sum(-1).view(-1, 1)  # vocab\n",
    "            text_emb_t = th.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1)  # d, bsz*seqlen\n",
    "            arr_norm = (text_emb ** 2).sum(-1).view(-1, 1)  # bsz*seqlen, 1\n",
    "            dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * th.mm(self.lm_head.weight,\n",
    "                                                                     text_emb_t)  # (vocab, d) x (d, bsz*seqlen)\n",
    "            scores = th.sqrt(th.clamp(dist, 0.0, np.inf)).view(emb_norm.size(0), hidden_repr.size(0),\n",
    "                                                               hidden_repr.size(1)) # vocab, bsz*seqlen\n",
    "            scores = -scores.permute(1, 2, 0).contiguous()\n",
    "            return scores\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"\n",
    "        Apply the model to an input batch.\n",
    "\n",
    "        :param x: an [N x C x ...] Tensor of inputs.\n",
    "        :param timesteps: a 1-D batch of timesteps.\n",
    "        :return: an [N x C x ...] Tensor of outputs.\n",
    "        \"\"\"\n",
    "        emb_t = self.time_embed(timestep_embedding(timesteps, self.hidden_t_dim))\n",
    "\n",
    "        if self.input_dims != self.hidden_size:\n",
    "            emb_x = self.input_up_proj(x)\n",
    "        else:\n",
    "            emb_x = x\n",
    "\n",
    "        seq_length = x.size(1)\n",
    "        position_ids = self.position_ids[:, : seq_length ]\n",
    "        # print(emb_x.shape, emb_t.shape, self.position_embeddings)\n",
    "        emb_inputs = self.position_embeddings(position_ids) + emb_x + emb_t.unsqueeze(1).expand(-1, seq_length, -1)\n",
    "        emb_inputs = self.dropout(self.LayerNorm(emb_inputs))\n",
    "\n",
    "        input_trans_hidden_states = self.input_transformers(emb_inputs).last_hidden_state\n",
    "        \n",
    "        if self.output_dims != self.hidden_size:\n",
    "            h = self.output_down_proj(input_trans_hidden_states)\n",
    "        else:\n",
    "            h = input_trans_hidden_states\n",
    "        h = h.type(x.dtype)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ebe3c",
   "metadata": {},
   "source": [
    "### step_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87323d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_named_schedule_sampler(name, diffusion):\n",
    "    \"\"\"\n",
    "    Create a ScheduleSampler from a library of pre-defined samplers.\n",
    "\n",
    "    :param name: the name of the sampler.\n",
    "    :param diffusion: the diffusion object to sample for.\n",
    "    \"\"\"\n",
    "    if name == \"uniform\":\n",
    "        return UniformSampler(diffusion)\n",
    "    elif name == \"lossaware\":\n",
    "        return LossSecondMomentResampler(diffusion)\n",
    "    elif name == \"fixstep\":\n",
    "        return FixSampler(diffusion)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"unknown schedule sampler: {name}\")\n",
    "\n",
    "\n",
    "class ScheduleSampler(ABC):\n",
    "    \"\"\"\n",
    "    A distribution over timesteps in the diffusion process, intended to reduce\n",
    "    variance of the objective.\n",
    "\n",
    "    By default, samplers perform unbiased importance sampling, in which the\n",
    "    objective's mean is unchanged.\n",
    "    However, subclasses may override sample() to change how the resampled\n",
    "    terms are reweighted, allowing for actual changes in the objective.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def weights(self):\n",
    "        \"\"\"\n",
    "        Get a numpy array of weights, one per diffusion step.\n",
    "\n",
    "        The weights needn't be normalized, but must be positive.\n",
    "        \"\"\"\n",
    "\n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Importance-sample timesteps for a batch. #(from diffuseq p.4)\n",
    "\n",
    "        :param batch_size: the number of timesteps.\n",
    "        :param device: the torch device to save to.\n",
    "        :return: a tuple (timesteps, weights):\n",
    "                 - timesteps: a tensor of timestep indices.\n",
    "                 - weights: a tensor of weights to scale the resulting losses.\n",
    "        \"\"\"\n",
    "        w = self.weights()\n",
    "        # print(f'[Bala] w: {w}')\n",
    "        p = w / np.sum(w)\n",
    "        indices_np = np.random.choice(len(p), size=(batch_size,), p=p)\n",
    "        indices = th.from_numpy(indices_np).long().to(device)\n",
    "        weights_np = 1 / (len(p) * p[indices_np])\n",
    "        weights = th.from_numpy(weights_np).float().to(device)\n",
    "        return indices, weights\n",
    "\n",
    "\n",
    "class UniformSampler(ScheduleSampler):\n",
    "    def __init__(self, diffusion_steps):\n",
    "        self.diffusion = diffusion_steps\n",
    "        self._weights = np.ones([diffusion_steps])\n",
    "\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "class FixSampler(ScheduleSampler):\n",
    "    def __init__(self, diffusion_steps):\n",
    "        self.diffusion_steps = diffusion_steps\n",
    "\n",
    "        ###############################################################\n",
    "        ### You can custome your own sampling weight of steps here. ###\n",
    "        ###############################################################\n",
    "        self._weights = np.concatenate([np.ones([diffusion_steps//2]), np.zeros([diffusion_steps//2]) + 0.5])\n",
    "\n",
    "    def weights(self):\n",
    "        return self._weights\n",
    "\n",
    "\n",
    "class LossAwareSampler(ScheduleSampler):\n",
    "    def update_with_local_losses(self, local_ts, local_losses):\n",
    "        \"\"\"\n",
    "        Update the reweighting using losses from a model.\n",
    "\n",
    "        Call this method from each rank with a batch of timesteps and the\n",
    "        corresponding losses for each of those timesteps.\n",
    "        This method will perform synchronization to make sure all of the ranks\n",
    "        maintain the exact same reweighting.\n",
    "\n",
    "        :param local_ts: an integer Tensor of timesteps.\n",
    "        :param local_losses: a 1D Tensor of losses.\n",
    "        \"\"\"\n",
    "        batch_sizes = [\n",
    "            th.tensor([0], dtype=th.int32, device=local_ts.device)\n",
    "            for _ in range(dist.get_world_size())\n",
    "        ]\n",
    "        # print(f'[Bala] batch_sizes: {batch_sizes}')\n",
    "        # print(f'[Bala] dist.get_world_size(): {dist.get_world_size()}')\n",
    "        dist.all_gather(\n",
    "            batch_sizes,\n",
    "            th.tensor([len(local_ts)], dtype=th.int32, device=local_ts.device),\n",
    "        )\n",
    "\n",
    "        # Pad all_gather batches to be the maximum batch size.\n",
    "        batch_sizes = [x.item() for x in batch_sizes]\n",
    "        max_bs = max(batch_sizes)\n",
    "        # print(f'[Bala] batch_sizes: {batch_sizes}')\n",
    "        # print(f'[Bala] max_bs: {max_bs}')\n",
    "        timestep_batches = [th.zeros(max_bs).to(local_ts) for bs in batch_sizes]\n",
    "        loss_batches = [th.zeros(max_bs).to(local_losses) for bs in batch_sizes]\n",
    "        # print(f'[Bala] loss_batches: {loss_batches}')\n",
    "        dist.all_gather(timestep_batches, local_ts)\n",
    "        dist.all_gather(loss_batches, local_losses)\n",
    "        timesteps = [\n",
    "            x.item() for y, bs in zip(timestep_batches, batch_sizes) for x in y[:bs]\n",
    "        ]\n",
    "        # print(f'[Bala] timesteps: {timesteps}') -> len = batch\n",
    "        losses = [x.item() for y, bs in zip(loss_batches, batch_sizes) for x in y[:bs]]\n",
    "        # print(f'[Bala] losses: {losses}') -> len = batch\n",
    "        self.update_with_all_losses(timesteps, losses)\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_with_all_losses(self, ts, losses):\n",
    "        \"\"\"\n",
    "        Update the reweighting using losses from a model.\n",
    "\n",
    "        Sub-classes should override this method to update the reweighting\n",
    "        using losses from the model.\n",
    "\n",
    "        This method directly updates the reweighting without synchronizing\n",
    "        between workers. It is called by update_with_local_losses from all\n",
    "        ranks with identical arguments. Thus, it should have deterministic\n",
    "        behavior to maintain state across workers.\n",
    "\n",
    "        :param ts: a list of int timesteps.\n",
    "        :param losses: a list of float losses, one per timestep.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class LossSecondMomentResampler(LossAwareSampler):\n",
    "    def __init__(self, diffusion_steps, history_per_term=10, uniform_prob=0.001):\n",
    "        self.diffusion_steps = diffusion_steps\n",
    "        self.history_per_term = history_per_term\n",
    "        self.uniform_prob = uniform_prob\n",
    "        self._loss_history = np.zeros(\n",
    "            [diffusion_steps, history_per_term], dtype=np.float64\n",
    "        )\n",
    "        self._loss_counts = np.zeros([diffusion_steps], dtype=np.int64)\n",
    "\n",
    "    def weights(self):\n",
    "        if not self._warmed_up():\n",
    "            return np.ones([self.diffusion_steps], dtype=np.float64)\n",
    "        weights = np.sqrt(np.mean(self._loss_history ** 2, axis=-1))\n",
    "        weights /= np.sum(weights)\n",
    "        weights *= 1 - self.uniform_prob\n",
    "        weights += self.uniform_prob / len(weights)\n",
    "        return weights\n",
    "\n",
    "    def update_with_all_losses(self, ts, losses):\n",
    "        for t, loss in zip(ts, losses):\n",
    "            if self._loss_counts[t] == self.history_per_term:\n",
    "                # Shift out the oldest loss term.\n",
    "                self._loss_history[t, :-1] = self._loss_history[t, 1:]\n",
    "                self._loss_history[t, -1] = loss\n",
    "            else:\n",
    "                self._loss_history[t, self._loss_counts[t]] = loss\n",
    "                self._loss_counts[t] += 1\n",
    "\n",
    "    def _warmed_up(self):\n",
    "        print(f'[Bala] _loss_counts: {self._loss_counts}')\n",
    "        print(f'[Bala] history_per_term: {self.history_per_term}')\n",
    "        print(f'[Bala] _warmed_up: {(self._loss_counts == self.history_per_term)}')\n",
    "        return (self._loss_counts == self.history_per_term).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c19aca",
   "metadata": {},
   "source": [
    "### train_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c664518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For ImageNet experiments, this was a good default value.\n",
    "# # We found that the lg_loss_scale quickly climbed to\n",
    "# # 20-21 within the first ~1K steps of training.\n",
    "# INITIAL_LOG_LOSS_SCALE = 20.0\n",
    "\n",
    "# class TrainLoop:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         *,\n",
    "#         model,\n",
    "#         diffusion,\n",
    "#         data,\n",
    "#         batch_size,\n",
    "#         microbatch,\n",
    "#         lr,\n",
    "#         ema_rate,\n",
    "#         log_interval,\n",
    "#         save_interval,\n",
    "#         resume_checkpoint,\n",
    "#         use_fp16=False,\n",
    "#         fp16_scale_growth=1e-3,\n",
    "#         schedule_sampler=None,\n",
    "#         weight_decay=0.0,\n",
    "#         learning_steps=0,\n",
    "#         checkpoint_path='',\n",
    "#         gradient_clipping=-1.,\n",
    "#         eval_data=None,\n",
    "#         eval_interval=-1,\n",
    "#     ):\n",
    "#         self.model = model\n",
    "#         self.diffusion = diffusion\n",
    "#         self.data = data\n",
    "#         self.eval_data = eval_data\n",
    "#         self.batch_size = batch_size\n",
    "#         self.microbatch = microbatch if microbatch > 0 else batch_size\n",
    "#         self.lr = lr\n",
    "#         self.ema_rate = (\n",
    "#             [ema_rate]\n",
    "#             if isinstance(ema_rate, float)\n",
    "#             else [float(x) for x in ema_rate.split(\",\")]\n",
    "#         )\n",
    "#         self.log_interval = log_interval\n",
    "#         self.eval_interval = eval_interval\n",
    "#         self.save_interval = save_interval\n",
    "#         self.resume_checkpoint = resume_checkpoint\n",
    "#         self.use_fp16 = use_fp16\n",
    "#         self.fp16_scale_growth = fp16_scale_growth\n",
    "#         self.schedule_sampler = schedule_sampler or UniformSampler(diffusion)\n",
    "#         self.weight_decay = weight_decay\n",
    "#         self.learning_steps = learning_steps\n",
    "#         self.gradient_clipping = gradient_clipping\n",
    "\n",
    "#         self.step = 0\n",
    "#         self.resume_step = 0\n",
    "#         self.global_batch = self.batch_size * dist.get_world_size()\n",
    "#         # print(f'[Bala] batch_size: {self.batch_size}')\n",
    "#         # print(f'[Bala] get_world_size: {dist.get_world_size()}')\n",
    "#         # print(f'[Bala] global_batch: {self.global_batch}')\n",
    "\n",
    "#         self.model_params = list(self.model.parameters())\n",
    "#         self.master_params = self.model_params\n",
    "#         self.lg_loss_scale = INITIAL_LOG_LOSS_SCALE\n",
    "#         self.sync_cuda = th.cuda.is_available()\n",
    "\n",
    "#         self.checkpoint_path = checkpoint_path # DEBUG **\n",
    "\n",
    "#         self._load_and_sync_parameters()\n",
    "#         if self.use_fp16:\n",
    "#             self._setup_fp16()\n",
    "\n",
    "#         self.opt = AdamW(self.master_params, lr=self.lr, weight_decay=self.weight_decay)\n",
    "#         if self.resume_step:\n",
    "#             # self._load_optimizer_state()\n",
    "#             frac_done = (self.step + self.resume_step) / self.learning_steps\n",
    "#             lr = self.lr * (1 - frac_done)\n",
    "#             self.opt = AdamW(self.master_params, lr=lr, weight_decay=self.weight_decay)\n",
    "#             # Model was resumed, either due to a restart or a checkpoint\n",
    "#             # being specified at the command line.\n",
    "#             self.ema_params = [\n",
    "#                 self._load_ema_parameters(rate) for rate in self.ema_rate\n",
    "#             ]\n",
    "#         else:\n",
    "#             self.ema_params = [\n",
    "#                 copy.deepcopy(self.master_params) for _ in range(len(self.ema_rate))\n",
    "#             ]\n",
    "\n",
    "#         if th.cuda.is_available(): # DEBUG **\n",
    "#             self.use_ddp = True\n",
    "#             print(dev())\n",
    "#             self.ddp_model = DDP(\n",
    "#                 self.model,\n",
    "#                 device_ids=[dev()],\n",
    "#                 output_device=dev(),\n",
    "#                 broadcast_buffers=False,\n",
    "#                 bucket_cap_mb=128,\n",
    "#                 find_unused_parameters=False,\n",
    "#             )\n",
    "#         else:\n",
    "#             if dist.get_world_size() > 1:\n",
    "#                 logger.warn(\n",
    "#                     \"Distributed training requires CUDA. \"\n",
    "#                     \"Gradients will not be synchronized properly!\"\n",
    "#                 )\n",
    "#             self.use_ddp = False\n",
    "#             self.ddp_model = self.model\n",
    "\n",
    "#     def _load_and_sync_parameters(self):\n",
    "#         resume_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n",
    "#         # print(f'[Bala] find_resume_checkpoint: {find_resume_checkpoint()}')\n",
    "#         # print(f'[Bala] self.resume_checkpoint: {self.resume_checkpoint}')\n",
    "#         # print(f'[Bala] resume_checkpoint: {resume_checkpoint}')\n",
    "\n",
    "#         if resume_checkpoint[-3:] == '.pt':\n",
    "#             self.resume_step = parse_resume_step_from_filename(resume_checkpoint)\n",
    "#             if dist.get_rank() == 0:\n",
    "#                 logger.log(f\"loading model from checkpoint: {resume_checkpoint}...\")\n",
    "#                 self.model.load_state_dict(\n",
    "#                     load_state_dict(\n",
    "#                         actual_model_path(resume_checkpoint), map_location=dev()\n",
    "#                     )\n",
    "#                 )\n",
    "\n",
    "#         # distruted broadcat parameters\n",
    "#         sync_params(self.model.parameters())\n",
    "\n",
    "#     def _load_ema_parameters(self, rate):\n",
    "#         ema_params = copy.deepcopy(self.master_params)\n",
    "\n",
    "#         main_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n",
    "#         ema_checkpoint = find_ema_checkpoint(main_checkpoint, self.resume_step, rate)\n",
    "#         if ema_checkpoint:\n",
    "#             if dist.get_rank() == 0:\n",
    "#                 logger.log(f\"loading EMA from checkpoint: {ema_checkpoint}...\")\n",
    "#                 state_dict = load_state_dict(\n",
    "#                     actual_model_path(ema_checkpoint), map_location=dev()\n",
    "#                 )\n",
    "#                 ema_params = self._state_dict_to_master_params(state_dict)\n",
    "\n",
    "#         sync_params(ema_params)\n",
    "#         return ema_params\n",
    "\n",
    "#     def _load_optimizer_state(self):\n",
    "#         main_checkpoint = find_resume_checkpoint() or self.resume_checkpoint\n",
    "#         if bf.exists(main_checkpoint):\n",
    "#             logger.log(f\"loading optimizer state from checkpoint: {main_checkpoint}\")\n",
    "#             state_dict = load_state_dict(\n",
    "#                 actual_model_path(main_checkpoint), map_location=dev()\n",
    "#             )\n",
    "#             self.opt.load_state_dict(state_dict)\n",
    "\n",
    "#     def _setup_fp16(self):\n",
    "#         self.master_params = make_master_params(self.model_params)\n",
    "#         self.model.convert_to_fp16()\n",
    "\n",
    "#     def run_loop(self):\n",
    "#         while (\n",
    "#             not self.learning_steps\n",
    "#             or self.step + self.resume_step < self.learning_steps\n",
    "#         ):\n",
    "#             # batch -> np.array of hidden_state / cond -> {input_ids: tensor, input_mask: tensor} \n",
    "#             batch, cond = next(self.data)\n",
    "#             # print(f'[Bala] batch.shape: {batch.shape}') (19, 128, 128)\n",
    "#             write_log(f'\\n[Train] step = {self.step}')\n",
    "#             self.run_step(batch, cond)\n",
    "#             # break\n",
    "#             if self.step % self.log_interval == 0:\n",
    "#                 logger.dumpkvs()\n",
    "#             if self.eval_data is not None and self.step % self.eval_interval == 0:\n",
    "#                 batch_eval, cond_eval = next(self.eval_data)\n",
    "#                 write_log(f'\\n[Vali] step = {self.step}')\n",
    "#                 self.forward_only(batch_eval, cond_eval)\n",
    "#                 print('eval on validation set')\n",
    "#                 logger.dumpkvs()\n",
    "#             if self.step > 0 and self.step % self.save_interval == 0:\n",
    "#                 self.save()\n",
    "#                 # Run for a finite amount of time in integration tests.\n",
    "#                 if os.environ.get(\"DIFFUSION_TRAINING_TEST\", \"\") and self.step > 0:\n",
    "#                     return\n",
    "#             self.step += 1\n",
    "#         # Save the last checkpoint if it wasn't already saved.\n",
    "#         if (self.step - 1) % self.save_interval != 0:\n",
    "#             self.save()\n",
    "\n",
    "#     def run_step(self, batch, cond):\n",
    "#         self.forward_backward(batch, cond)\n",
    "#         if self.use_fp16:\n",
    "#             self.optimize_fp16()\n",
    "#         else:\n",
    "#             self.optimize_normal()\n",
    "#         self.log_step()\n",
    "\n",
    "#     def forward_only(self, batch, cond):\n",
    "#         with th.no_grad():\n",
    "#             zero_grad(self.model_params)\n",
    "#             for i in range(0, batch.shape[0], self.microbatch):\n",
    "#                 micro = batch[i: i + self.microbatch].to(dev())\n",
    "#                 micro_cond = {\n",
    "#                     k: v[i: i + self.microbatch].to(dev())\n",
    "#                     for k, v in cond.items()\n",
    "#                 }\n",
    "#                 last_batch = (i + self.microbatch) >= batch.shape[0]\n",
    "#                 t, weights = self.schedule_sampler.sample(micro.shape[0], dev())\n",
    "#                 # print(micro_cond.keys())\n",
    "#                 compute_losses = functools.partial(\n",
    "#                     self.diffusion.training_losses,\n",
    "#                     self.ddp_model,\n",
    "#                     micro,\n",
    "#                     t,\n",
    "#                     model_kwargs=micro_cond,\n",
    "#                 )\n",
    "\n",
    "#                 if last_batch or not self.use_ddp:\n",
    "#                     losses = compute_losses()\n",
    "#                 else:\n",
    "#                     with self.ddp_model.no_sync():\n",
    "#                         losses = compute_losses()\n",
    "\n",
    "#                 if i + self.microbatch >= batch.shape[0]:\n",
    "#                     log_loss_dict(\n",
    "#                         self.diffusion, t, {f\"eval_{k}\": v * weights for k, v in losses.items()},\n",
    "#                         True\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     log_loss_dict(\n",
    "#                         self.diffusion, t, {f\"eval_{k}\": v * weights for k, v in losses.items()},\n",
    "#                         False\n",
    "#                     )\n",
    "\n",
    "\n",
    "#     def forward_backward(self, batch, cond):\n",
    "#         zero_grad(self.model_params)\n",
    "#         for i in range(0, batch.shape[0], self.microbatch):\n",
    "#             micro = batch[i : i + self.microbatch].to(dev())\n",
    "#             micro_cond = {\n",
    "#                 k: v[i : i + self.microbatch].to(dev())\n",
    "#                 for k, v in cond.items()\n",
    "#             }\n",
    "#             last_batch = (i + self.microbatch) >= batch.shape[0]\n",
    "#             t, weights = self.schedule_sampler.sample(micro.shape[0], dev())\n",
    "#             # print(f'[Bala] t: {t}')\n",
    "#             # print(f'[Bala] weights: {weights}')\n",
    "#             # print(micro_cond.keys())\n",
    "#             compute_losses = functools.partial(\n",
    "#                 self.diffusion.training_losses,\n",
    "#                 self.ddp_model,\n",
    "#                 micro,\n",
    "#                 t,\n",
    "#                 model_kwargs=micro_cond,\n",
    "#             )\n",
    "\n",
    "#             if last_batch or not self.use_ddp:\n",
    "#                 losses = compute_losses()\n",
    "#             else:\n",
    "#                 with self.ddp_model.no_sync():\n",
    "#                     losses = compute_losses()\n",
    "\n",
    "#             if isinstance(self.schedule_sampler, LossAwareSampler):\n",
    "#                 self.schedule_sampler.update_with_local_losses(\n",
    "#                     t, losses[\"loss\"].detach()\n",
    "#                 )\n",
    "\n",
    "#             # loss -> scalar\n",
    "#             loss = (losses[\"loss\"] * weights).mean()\n",
    "#             if i + self.microbatch >= batch.shape[0]:\n",
    "#                 log_loss_dict(\n",
    "#                     self.diffusion, t, {k: v * weights for k, v in losses.items()},\n",
    "#                     True\n",
    "#                 )\n",
    "#             else:\n",
    "#                 log_loss_dict(\n",
    "#                     self.diffusion, t, {k: v * weights for k, v in losses.items()},\n",
    "#                     False\n",
    "#                 )\n",
    "#             if self.use_fp16:\n",
    "#                 loss_scale = 2 ** self.lg_loss_scale\n",
    "#                 (loss * loss_scale).backward()\n",
    "#             else:\n",
    "#                 loss.backward()\n",
    "\n",
    "#     def optimize_fp16(self):\n",
    "#         if any(not th.isfinite(p.grad).all() for p in self.model_params):\n",
    "#             self.lg_loss_scale -= 1\n",
    "#             logger.log(f\"Found NaN, decreased lg_loss_scale to {self.lg_loss_scale}\")\n",
    "#             return\n",
    "\n",
    "#         model_grads_to_master_grads(self.model_params, self.master_params)\n",
    "#         self.master_params[0].grad.mul_(1.0 / (2 ** self.lg_loss_scale))\n",
    "#         self._log_grad_norm()\n",
    "#         self._anneal_lr()\n",
    "#         self.opt.step()\n",
    "#         for rate, params in zip(self.ema_rate, self.ema_params):\n",
    "#             update_ema(params, self.master_params, rate=rate)\n",
    "#         master_params_to_model_params(self.model_params, self.master_params)\n",
    "#         self.lg_loss_scale += self.fp16_scale_growth\n",
    "\n",
    "#     def grad_clip(self):\n",
    "#         # print('doing gradient clipping')\n",
    "#         max_grad_norm=self.gradient_clipping #3.0\n",
    "#         if hasattr(self.opt, \"clip_grad_norm\"):\n",
    "#             # Some optimizers (like the sharded optimizer) have a specific way to do gradient clipping\n",
    "#             self.opt.clip_grad_norm(max_grad_norm)\n",
    "#         # else:\n",
    "#         #     assert False\n",
    "#         # elif hasattr(self.model, \"clip_grad_norm_\"):\n",
    "#         #     # Some models (like FullyShardedDDP) have a specific way to do gradient clipping\n",
    "#         #     self.model.clip_grad_norm_(args.max_grad_norm)\n",
    "#         else:\n",
    "#             # Revert to normal clipping otherwise, handling Apex or full precision\n",
    "#             th.nn.utils.clip_grad_norm_(\n",
    "#                 self.model.parameters(), #amp.master_params(self.opt) if self.use_apex else\n",
    "#                 max_grad_norm,\n",
    "#             )\n",
    "\n",
    "#     def optimize_normal(self):\n",
    "#         if self.gradient_clipping > 0:\n",
    "#             self.grad_clip()\n",
    "#         self._log_grad_norm()\n",
    "#         self._anneal_lr()\n",
    "#         self.opt.step()\n",
    "#         for rate, params in zip(self.ema_rate, self.ema_params):\n",
    "#             update_ema(params, self.master_params, rate=rate)\n",
    "\n",
    "#     def _log_grad_norm(self):\n",
    "#         sqsum = 0.0\n",
    "#         # cnt = 0\n",
    "#         for p in self.master_params:\n",
    "#             # print(cnt, p) ## DEBUG\n",
    "#             # print(cnt, p.grad)\n",
    "#             # cnt += 1\n",
    "#             if p.grad != None:\n",
    "#                 sqsum += (p.grad ** 2).sum().item()\n",
    "#         logger.logkv_mean(\"grad_norm\", np.sqrt(sqsum), checkpoint_path, True)\n",
    "\n",
    "#     def _anneal_lr(self):\n",
    "#         if not self.learning_steps:\n",
    "#             return\n",
    "#         frac_done = (self.step + self.resume_step) / self.learning_steps\n",
    "#         lr = self.lr * (1 - frac_done)\n",
    "#         for param_group in self.opt.param_groups:\n",
    "#             param_group[\"lr\"] = lr\n",
    "\n",
    "#     def log_step(self):\n",
    "#         logger.logkv(\"step\", self.step + self.resume_step)\n",
    "#         logger.logkv(\"samples\", (self.step + self.resume_step + 1) * self.global_batch)\n",
    "#         if self.use_fp16:\n",
    "#             logger.logkv(\"lg_loss_scale\", self.lg_loss_scale)\n",
    "\n",
    "#     def save(self):\n",
    "#         def save_checkpoint(rate, params):\n",
    "#             state_dict = self._master_params_to_state_dict(params)\n",
    "#             if dist.get_rank() == 0:\n",
    "#                 logger.log(f\"saving model {rate}...\")\n",
    "#                 if not rate:\n",
    "#                     filename = f\"model{(self.step+self.resume_step):06d}.pt\"\n",
    "#                 else:\n",
    "#                     filename = f\"ema_{rate}_{(self.step+self.resume_step):06d}.pt\"\n",
    "#                 print('writing to', bf.join(get_blob_logdir(), filename))\n",
    "#                 print('writing to', bf.join(self.checkpoint_path, filename))\n",
    "#                 # with bf.BlobFile(bf.join(get_blob_logdir(), filename), \"wb\") as f:\n",
    "#                 #     th.save(state_dict, f)\n",
    "#                 with bf.BlobFile(bf.join(self.checkpoint_path, filename), \"wb\") as f: # DEBUG **\n",
    "#                     th.save(state_dict, f) # save locally\n",
    "#                     # pass # save empty\n",
    "\n",
    "#         # save_checkpoint(0, self.master_params)\n",
    "#         for rate, params in zip(self.ema_rate, self.ema_params):\n",
    "#             save_checkpoint(rate, params)\n",
    "\n",
    "#         dist.barrier()\n",
    "\n",
    "#     def _master_params_to_state_dict(self, master_params):\n",
    "#         if self.use_fp16:\n",
    "#             master_params = unflatten_master_params(\n",
    "#                 list(self.model.parameters()), master_params # DEBUG **\n",
    "#             )\n",
    "#         state_dict = self.model.state_dict()\n",
    "#         for i, (name, _value) in enumerate(self.model.named_parameters()):\n",
    "#             assert name in state_dict\n",
    "#             state_dict[name] = master_params[i]\n",
    "#         return state_dict\n",
    "\n",
    "#     def _state_dict_to_master_params(self, state_dict):\n",
    "#         params = [state_dict[name] for name, _ in self.model.named_parameters()]\n",
    "#         if self.use_fp16:\n",
    "#             return make_master_params(params)\n",
    "#         else:\n",
    "#             return params\n",
    "\n",
    "\n",
    "# def parse_resume_step_from_filename(filename):\n",
    "#     \"\"\"\n",
    "#     Parse filenames of the form path/to/modelNNNNNN.pt, where NNNNNN is the\n",
    "#     checkpoint's number of steps.\n",
    "#     \"\"\"\n",
    "#     if filename[-3:] == '.pt':\n",
    "#         return int(filename[-9:-3])\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "\n",
    "# def get_blob_logdir():\n",
    "#     return os.environ.get(\"DIFFUSION_BLOB_LOGDIR\", logger.get_dir())\n",
    "\n",
    "\n",
    "# def find_resume_checkpoint():\n",
    "#     # On your infrastructure, you may want to override this to automatically\n",
    "#     # discover the latest checkpoint on your blob storage, etc.\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def find_ema_checkpoint(main_checkpoint, step, rate):\n",
    "#     if main_checkpoint is None:\n",
    "#         return None\n",
    "#     filename = f\"ema_{rate}_{(step):06d}.pt\"\n",
    "#     path = bf.join(bf.dirname(main_checkpoint), filename)\n",
    "#     if bf.exists(path):\n",
    "#         return path\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def log_loss_dict(diffusion, ts, losses, writeFile):\n",
    "#     for key, values in losses.items():\n",
    "#         logger.logkv_mean(key, values.mean().item(), checkpoint_path, writeFile)\n",
    "#         # Log the quantiles (four quartiles, in particular).\n",
    "#         for sub_t, sub_loss in zip(ts.cpu().numpy(), values.detach().cpu().numpy()):\n",
    "#             quartile = int(4 * sub_t / diffusion.num_timesteps)\n",
    "#             logger.logkv_mean(f\"{key}_q{quartile}\", sub_loss, checkpoint_path, writeFile)\n",
    "    \n",
    "\n",
    "# def actual_model_path(model_path):\n",
    "#     return model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ee6cd",
   "metadata": {},
   "source": [
    "### rounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d8b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_efficient_knn(model_emb, text_emb):\n",
    "    # print(f'[Bala] model_emb: {model_emb.shape}') -> (vocab, emb_dim)\n",
    "    emb_norm = (model_emb**2).sum(-1).view(-1, 1) # vocab\n",
    "    # print(f'[Bala] emb_norm: {emb_norm.shape}') -> (vocab, 1)\n",
    "    # print(f'[Bala] text_emb: {text_emb.shape}') -> (bsz*seqlen, emb_dim)\n",
    "    text_emb_t = th.transpose(text_emb.view(-1, text_emb.size(-1)), 0, 1) # d, bsz*seqlen\n",
    "    arr_norm = (text_emb ** 2).sum(-1).view(-1, 1) # bsz*seqlen, 1\n",
    "    # print(f'[Bala] text_emb_t: {text_emb_t.shape}') -> (emb_dim, bsz*seqlen)\n",
    "    # print(f'[Bala] arr_norm: {arr_norm.shape}') -> (bsz*seqlen, 1)\n",
    "    # print(emb_norm.shape, arr_norm.shape)\n",
    "    dist = emb_norm + arr_norm.transpose(0, 1) - 2.0 * th.mm(model_emb, text_emb_t) # (vocab, d) x (d, bsz*seqlen)\n",
    "    # print(f'[Bala] dist: {dist.shape}\\n{dist}') -> (vocab, bsz*seqlen)\n",
    "    dist = th.clamp(dist, 0.0, np.inf)\n",
    "    # print(dist.shape)\n",
    "    topk_out = th.topk(-dist, k=1, dim=0)\n",
    "    return topk_out.values, topk_out.indices\n",
    "\n",
    "def get_weights(model, emb_scale_factor):\n",
    "    if hasattr(model, 'transformer'):\n",
    "        input_embs = model.transformer.wte  # input_embs\n",
    "        down_proj = model.down_proj\n",
    "        model_emb = down_proj(input_embs.weight)\n",
    "        print(model_emb.shape)\n",
    "        model = th.nn.Embedding(model_emb.size(0), model_emb.size(1))\n",
    "        print(emb_scale_factor)\n",
    "        model.weight.data = model_emb * emb_scale_factor\n",
    "\n",
    "    elif hasattr(model, 'weight'):\n",
    "        pass\n",
    "    else:\n",
    "        assert NotImplementedError\n",
    "        \n",
    "    model.weight.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def denoised_fn_round(model, text_emb, t):\n",
    "    # :params text_emb: model output\n",
    "    # print('text_emb', text_emb.shape) # bsz, seqlen, dim\n",
    "    model_emb = model.weight  # input_embs\n",
    "    # print('t', t)\n",
    "    old_shape = text_emb.shape\n",
    "    old_device = text_emb.device\n",
    "\n",
    "    if len(text_emb.shape) > 2:\n",
    "        text_emb = text_emb.reshape(-1, text_emb.size(-1)) # (bsz * seqlen, dim)\n",
    "    else:\n",
    "        text_emb = text_emb\n",
    "    # val, indices = get_knn(model_emb, text_emb.to(model_emb.device), dist=dist)\n",
    "    val, indices = get_efficient_knn(model_emb, text_emb.to(model_emb.device))\n",
    "    rounded_tokens = indices[0]\n",
    "    # print(f'[Bala] rounded_tokens: {rounded_tokens}') -> (bsz * seqlen)\n",
    "    # print('rounded_tokens', rounded_tokens.shape, rounded_tokens)\n",
    "    new_embeds = model(rounded_tokens).view(old_shape).to(old_device)\n",
    "    # print(f'[Bala] new_embeds: {new_embeds.shape}') -> (bsz, seqlen, emb_dim)\n",
    "\n",
    "    return new_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f8065f",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe503500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(recover, reference):\n",
    "    return sentence_bleu([recover.split()], reference.split(), smoothing_function=SmoothingFunction().method4,)\n",
    "\n",
    "def selectBest(sentences):\n",
    "    selfBleu = [[] for i in range(len(sentences))]\n",
    "    for i, s1 in enumerate(sentences):\n",
    "        for j, s2 in enumerate(sentences):\n",
    "            score = get_bleu(s1, s2)\n",
    "            selfBleu[i].append(score)\n",
    "    for i, s1 in enumerate(sentences):\n",
    "        selfBleu[i][i] = 0\n",
    "    idx = np.argmax(np.sum(selfBleu, -1))\n",
    "    return sentences[idx]\n",
    "\n",
    "def diversityOfSet(sentences):\n",
    "    selfBleu = []\n",
    "    # print(sentences)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j in range(i+1, len(sentences)):\n",
    "            # print(sentence, sentences[j])\n",
    "            score = get_bleu(sentence, sentences[j])\n",
    "            selfBleu.append(score)\n",
    "    if len(selfBleu)==0:\n",
    "        selfBleu.append(0)\n",
    "    div4 = distinct_n_gram_inter_sent(sentences, 4)\n",
    "    return np.mean(selfBleu), div4\n",
    "\n",
    "\n",
    "def distinct_n_gram(hypn,n):\n",
    "    dist_list = []\n",
    "    for hyp in hypn:\n",
    "        hyp_ngrams = []\n",
    "        hyp_ngrams += nltk.ngrams(hyp.split(), n)\n",
    "        total_ngrams = len(hyp_ngrams)\n",
    "        unique_ngrams = len(list(set(hyp_ngrams)))\n",
    "        if total_ngrams == 0:\n",
    "            return 0\n",
    "        dist_list.append(unique_ngrams/total_ngrams)\n",
    "    return  np.mean(dist_list)\n",
    "\n",
    "\n",
    "def distinct_n_gram_inter_sent(hypn, n):\n",
    "    hyp_ngrams = []\n",
    "    for hyp in hypn:\n",
    "        hyp_ngrams += nltk.ngrams(hyp.split(), n)\n",
    "    total_ngrams = len(hyp_ngrams)\n",
    "    unique_ngrams = len(list(set(hyp_ngrams)))\n",
    "    if total_ngrams == 0:\n",
    "        return 0\n",
    "    dist_n = unique_ngrams/total_ngrams\n",
    "    return  dist_n\n",
    "\n",
    "def metrics_calculate(recovers, sources, references):\n",
    "    bleu = []\n",
    "    rougel = []\n",
    "    avg_len = []\n",
    "    dist1 = []\n",
    "    \n",
    "    sos = '[CLS]'\n",
    "    eos = '[SEP]'\n",
    "    sep = '[SEP]'\n",
    "    pad = '[PAD]'\n",
    "    \n",
    "    rougeScore = ROUGEScore()\n",
    "    \n",
    "    for i in range(len(sources)):\n",
    "        sources[i] = sources[i].replace(eos, '').replace(sos, '')\n",
    "        references[i] = references[i].replace(eos, '').replace(sos, '').replace(sep, '')\n",
    "        recovers[i] = recovers[i].replace(eos, '').replace(sos, '').replace(sep, '').replace(pad, '')\n",
    "        \n",
    "        avg_len.append(len(recovers[i].split(' ')))\n",
    "        bleu.append(get_bleu(recovers[i], references[i]))\n",
    "        rougel.append(rougeScore(recovers[i], references[i])['rougeL_fmeasure'].tolist())\n",
    "        dist1.append(distinct_n_gram([recovers[i]], 1))\n",
    "        \n",
    "    P, R, F1 = score(recovers, references, model_type='microsoft/deberta-xlarge-mnli', lang='en', verbose=True)\n",
    "    \n",
    "    bleu = np.mean(bleu)\n",
    "    rougel = np.mean(rougel)\n",
    "    F1 = th.mean(F1)\n",
    "    dist1 = np.mean(dist1)\n",
    "    avg_len = np.mean(avg_len)\n",
    "    # print('avg BLEU score', bleu)\n",
    "    # print('avg ROUGE-L score', rougel)\n",
    "    # print('avg berscore', F1)\n",
    "    # print('avg dist1 score', dist1)\n",
    "    # print('avg len',avg_len)\n",
    "    return bleu, rougel, F1, dist1, avg_len\n",
    "\n",
    "def metrics_calculate_S(recovers_S, sources_S, references_S, S):\n",
    "    bleu = []\n",
    "    rougel = []\n",
    "    avg_len = []\n",
    "    dist1 = []\n",
    "    \n",
    "    sos = '[CLS]'\n",
    "    eos = '[SEP]'\n",
    "    sep = '[SEP]'\n",
    "    pad = '[PAD]'\n",
    "    \n",
    "    rougeScore = ROUGEScore()\n",
    "    \n",
    "    sentenceDict = {}\n",
    "    referenceDict = {}\n",
    "    sourceDict = {}\n",
    "    for i in range(len(sources_S[0])):\n",
    "        sentenceDict[i] = []\n",
    "        referenceDict[i] = []\n",
    "        sourceDict[i] = []\n",
    "    \n",
    "    for s in range(S):\n",
    "        sources = sources_S.pop()\n",
    "        references = references_S.pop()\n",
    "        recovers = recovers_S.pop()\n",
    "        for i in range(len(sources)):\n",
    "            sources[i] = sources[i].replace(eos, '').replace(sos, '')\n",
    "            references[i] = references[i].replace(eos, '').replace(sos, '').replace(sep, '')\n",
    "            recovers[i] = recovers[i].replace(eos, '').replace(sos, '').replace(sep, '').replace(pad, '')\n",
    "\n",
    "            sentenceDict[i].append(recovers[i])\n",
    "            referenceDict[i].append(references[i])\n",
    "            sourceDict[i].append(sources[i])\n",
    "    \n",
    "    # diversity\n",
    "    div4 = []\n",
    "    selfBleu = []\n",
    "    for k, v in sentenceDict.items():\n",
    "        if len(v) == 0:\n",
    "            continue\n",
    "        sb, d4 = diversityOfSet(v)\n",
    "        selfBleu.append(sb)\n",
    "        div4.append(d4)\n",
    "\n",
    "    selfBleu = np.mean(selfBleu)\n",
    "    div4 = np.mean(div4)\n",
    "    # print('avg selfBleu score', selfBleu)\n",
    "    # print('avg div4 score', div4)\n",
    "    \n",
    "    bleu = []\n",
    "    rougel = []\n",
    "    avg_len = []\n",
    "    dist1 = []\n",
    "    recovers = []\n",
    "    references = []\n",
    "    sources = []\n",
    "\n",
    "    for k, v in sentenceDict.items():\n",
    "        if len(v) == 0 or len(referenceDict[k]) == 0:\n",
    "            continue\n",
    "\n",
    "        recovers.append(selectBest(v))\n",
    "        references.append(referenceDict[k][0])\n",
    "        sources.append(sourceDict[k][0])\n",
    "\n",
    "    for (source, reference, recover) in zip(sources, references, recovers):\n",
    "        bleu.append(get_bleu(recover, reference))\n",
    "        rougel.append(rougeScore(recover, reference)['rougeL_fmeasure'].tolist())\n",
    "        avg_len.append(len(recover.split(' ')))\n",
    "        dist1.append(distinct_n_gram([recover], 1))\n",
    "\n",
    "    P, R, F1 = score(recovers, references, model_type='microsoft/deberta-xlarge-mnli', lang='en', verbose=True)\n",
    "\n",
    "    bleu = np.mean(bleu)\n",
    "    rougel = np.mean(rougel)\n",
    "    F1 = th.mean(F1)\n",
    "    dist1 = np.mean(dist1)\n",
    "    avg_len = np.mean(avg_len)\n",
    "    # print('avg BLEU score', bleu)\n",
    "    # print('avg ROUGE-L score', rougel)\n",
    "    # print('avg berscore', F1)\n",
    "    # print('avg dist1 score', dist1)\n",
    "    return bleu, rougel, F1, dist1, selfBleu, div4, avg_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644d5204",
   "metadata": {},
   "source": [
    "### Model B  (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b875d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True) \n",
    "        \n",
    "    def forward(self, input_sentences):\n",
    "        embedded = self.embedding(input_sentences)\n",
    "        encoder_output, (encoder_h, encoder_c) = self.lstm(embedded)\n",
    "        return encoder_output, encoder_h, encoder_c\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_size+1, embedding_dim) # output_size+1 -> T/F + BOS\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, input_instructions, encoder_h, encoder_c):\n",
    "        embedded = self.embedding(input_instructions)\n",
    "        decoder_output, (decoder_h, decoder_c) = self.lstm(embedded, (encoder_h, encoder_c))\n",
    "        output = self.linear(decoder_output)\n",
    "        output = self.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b6748",
   "metadata": {},
   "source": [
    "### other functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19879e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_log_file(filename):\n",
    "    with open(f'{checkpoint_path}/{filename}.csv', 'w' , newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "\n",
    "def write_log(msg):\n",
    "    with open(f'{checkpoint_path}/training_log.csv', 'a' , newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([msg])\n",
    "        \n",
    "def write_time_usage(msg):\n",
    "    with open(f'{checkpoint_path}/time_usage.csv', 'a' , newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([msg])\n",
    "        \n",
    "def record_information(checkpoint_path, msg, filename):\n",
    "    with open(f'{checkpoint_path}/{filename}.csv', 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([msg])\n",
    "        \n",
    "def sample_sentence(data, mask, num, N, seed = None):\n",
    "    idx_list = []\n",
    "    if seed != None:\n",
    "        random.seed(seed)\n",
    "    while(len(idx_list) < ((num-1) // N + 1)):\n",
    "        idx = random.randint(0, len(data)-1)\n",
    "        if idx not in idx_list: idx_list.append(idx)\n",
    "    batch_data = th.tensor([data[idx] for idx in idx_list for _ in range(N)])[:num]\n",
    "    batch_mask = th.tensor([mask[idx] for idx in idx_list for _ in range(N)])[:num]\n",
    "    return batch_data, batch_mask, idx_list\n",
    "\n",
    "def mask_y(data, mask):\n",
    "    x = []\n",
    "    for i in range(len(data)):\n",
    "        y_mask = np.array([1-instruction for instruction in mask[i]])\n",
    "        x.append(list(np.array(data[i])*y_mask))\n",
    "    x = th.tensor(x)\n",
    "    return x\n",
    "\n",
    "def get_standard_betas(diffusion_steps, noise_schedule):\n",
    "    if noise_schedule == 'linear':\n",
    "        scale = 1000 / diffusion_steps\n",
    "        beta_start = scale * 0.0001\n",
    "        beta_end = scale * 0.02\n",
    "        return np.linspace(beta_start, beta_end, diffusion_steps, dtype=np.float64)\n",
    "    elif noise_schedule == 'sqrt':\n",
    "        return betas_for_alpha_bar(diffusion_steps, lambda t: 1-np.sqrt(t + 0.0001),)\n",
    "\n",
    "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
    "    \"\"\"\n",
    "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
    "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
    "\n",
    "    :param num_diffusion_timesteps: the number of betas to produce.\n",
    "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
    "                      produces the cumulative product of (1-beta) up to that\n",
    "                      part of the diffusion process.\n",
    "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
    "                     prevent singularities.\n",
    "    \"\"\"\n",
    "    betas = []\n",
    "    for i in range(num_diffusion_timesteps):\n",
    "        t1 = i / num_diffusion_timesteps\n",
    "        t2 = (i + 1) / num_diffusion_timesteps\n",
    "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
    "    return np.array(betas)\n",
    "\n",
    "def calculate_my_betas(beta_ins, standard_betas):\n",
    "    # beta_ins -> (batch, diffusion_steps), standard_betas -> (diffusion_steps, )\n",
    "    betas = list()\n",
    "#     for i in range(len(beta_ins)):\n",
    "#         standard_betas_copy = standard_betas.copy()\n",
    "#         beta_now, standard_betas_copy = standard_betas_copy[0], standard_betas_copy[1:]\n",
    "#         beta_temp = [beta_now]\n",
    "#         for instruction in beta_ins[i]:\n",
    "#             if instruction: beta_now, standard_betas_copy = standard_betas_copy[0], standard_betas_copy[1:]\n",
    "#             beta_temp.append(beta_now)\n",
    "#         betas.append(beta_temp)\n",
    "    \n",
    "    for i in range(len(beta_ins)):\n",
    "        point = 0\n",
    "        now = 0\n",
    "        standard_betas_copy = standard_betas.copy()\n",
    "        beta_temp = [standard_betas_copy[point]]\n",
    "        point += 1\n",
    "        now = point\n",
    "        for instruction in beta_ins[i]:\n",
    "            if instruction: \n",
    "                beta_temp.append(standard_betas_copy[point])\n",
    "                now = point\n",
    "            else: \n",
    "                beta_temp.append(standard_betas_copy[now])\n",
    "            point += 1\n",
    "        betas.append(beta_temp)\n",
    "    betas = th.tensor(betas)\n",
    "    return betas\n",
    "\n",
    "# 找不到為啥要把diffusion step rescale到0-1000\n",
    "def _scale_timesteps(t, rescale_timesteps, num_timesteps):\n",
    "    if rescale_timesteps:\n",
    "        return t.float() * (1000.0 / num_timesteps)\n",
    "    return t\n",
    "\n",
    "def modelB_inference(batch_data, encoder, decoder, diffusion_steps, word2idx, mode):\n",
    "    # diffusion_steps-1 => 只需要diffusion_steps-1個instructions，來得到diffusion_steps個betas\n",
    "    s_t = time.time()\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with th.no_grad():\n",
    "        encoder_output, encoder_h, encoder_c = encoder(batch_data.to(dev()))\n",
    "\n",
    "        decoder_input = th.tensor([[word2idx['BOS']]*(diffusion_steps-1) for _ in range(batch_data.shape[0])])\n",
    "        # for i in range(decoder_input.shape[0]): decoder_input[i][0] = word2idx['BOS']\n",
    "        decoder_input = decoder_input.to(dev())\n",
    "\n",
    "        inference_data = th.tensor([[word2idx['F']]*(diffusion_steps-1) for _ in range(decoder_input.shape[0])])\n",
    "        for step in range(diffusion_steps-1):\n",
    "            decoder_output = decoder(decoder_input, encoder_h, encoder_c)\n",
    "            decoder_output = decoder_output[:,step,]\n",
    "            if mode == 'argmax':\n",
    "                decoder_output_TF = th.argmax(decoder_output, dim=1)\n",
    "            elif mode == 'multinomial':\n",
    "                decoder_output_TF = th.multinomial(decoder_output, 1)\n",
    "                decoder_output_TF = decoder_output_TF[:,0]\n",
    "\n",
    "            inference_data[:, step] = decoder_output_TF\n",
    "            if step+1 < diffusion_steps-1:\n",
    "                decoder_input[:,step+1] = decoder_output_TF\n",
    "            del decoder_output, decoder_output_TF\n",
    "            # th.cuda.empty_cache()\n",
    "        del encoder_output, encoder_h, encoder_c, decoder_input\n",
    "        # th.cuda.empty_cache()\n",
    "    e_t = time.time() - s_t\n",
    "    write_time_usage(f'[Model B Inference] batch:{batch_data.shape[0]}. Using {e_t/60:.2f} mins.')\n",
    "    return inference_data\n",
    "\n",
    "def get_loss_reward(\n",
    "    betas, \n",
    "    data, \n",
    "    mask, \n",
    "    model, \n",
    "    diffusion_steps,\n",
    "    t,\n",
    "    timestep_respacing, \n",
    "    rescale_timesteps,\n",
    "    predict_xstart,\n",
    "    learn_sigma,\n",
    "    use_kl,\n",
    "    rescale_learned_sigmas,\n",
    "):\n",
    "    s_t = time.time()\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        # create diffusion with betas\n",
    "        if not timestep_respacing:\n",
    "            timestep_respacing = [diffusion_steps]\n",
    "\n",
    "        diffusion = SpacedDiffusion(\n",
    "            use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "            betas=betas,\n",
    "            rescale_timesteps=rescale_timesteps,\n",
    "            predict_xstart=predict_xstart,\n",
    "            learn_sigmas = learn_sigma,\n",
    "            sigma_small = sigma_small,\n",
    "            use_kl = use_kl,\n",
    "            rescale_learned_sigmas=rescale_learned_sigmas\n",
    "        )\n",
    "\n",
    "        # get q sample with each diffusion_steps\n",
    "        x_start_mean = model.module.get_embeds(data) # shape -> (batch_size, seq_len, hidden_dim)\n",
    "        std = _extract_into_tensor(\n",
    "            diffusion.sqrt_one_minus_alphas_cumprod,\n",
    "            th.tensor([0]*x_start_mean.shape[0]).to(x_start_mean.device),\n",
    "            x_start_mean.shape\n",
    "        ).to(dev())\n",
    "\n",
    "        x_start_q = diffusion._get_x_start(x_start_mean, std) # shape -> (batch_size, seq_len, hidden_dim)\n",
    "        q_mask = mask\n",
    "\n",
    "        noise = th.randn_like(x_start_q)\n",
    "\n",
    "        # get_logits = model.module.get_logits\n",
    "        \n",
    "        # calculate each diffusion_steps loss\n",
    "        target = x_start_q\n",
    "        \n",
    "        reward_loss = []\n",
    "        # reward_loss = th.zeros(len(data), diffusion_steps) # (batch, diffusion_steps)\n",
    "        # t = th.tensor([i]*x_start_q.shape[0]).to(dev())\n",
    "        # t, weights = schedule_sampler.sample(x_start_q.shape[0], dev())\n",
    "        q = diffusion.q_sample(x_start_q, t, noise=noise, mask=q_mask).to(dev())\n",
    "        model_output = model(q, _scale_timesteps(t, rescale_timesteps, diffusion_steps))\n",
    "        # model_output = model(q, t)\n",
    "\n",
    "        mse = mean_flat((target - model_output) ** 2)\n",
    "\n",
    "        t0_mask = (t == 0)\n",
    "        t0_loss = mean_flat((x_start_mean - model_output) ** 2)\n",
    "        mse = th.where(t0_mask, t0_loss, mse)\n",
    "        # mse越小，代表denoise結果越好 -> reward就應該要變大\n",
    "        mse = 1.0 / mse\n",
    "\n",
    "        # for i in range(len(t)):\n",
    "        #     reward_loss[i][t[i]] = mse[i]\n",
    "        reward_loss.append(mse.tolist())\n",
    "\n",
    "        del mse, t, q, model_output, t0_mask, t0_loss\n",
    "        # del out_mean, tT_loss, decoder_nll, loss\n",
    "        th.cuda.empty_cache()\n",
    "\n",
    "        reward_loss = th.tensor(reward_loss)\n",
    "        # shape: (diffusion_steps, batch) -> (batch, diffusion_steps)\n",
    "        reward_loss = th.transpose(reward_loss, 0, 1)\n",
    "        del x_start_mean, std, x_start_q, q_mask, noise, target\n",
    "        th.cuda.empty_cache()\n",
    "    e_t = time.time() - s_t\n",
    "    write_time_usage(f'[Get loss reward] batch:{data.shape[0]}. Using {e_t/60:.2f} mins.')\n",
    "    return reward_loss\n",
    "\n",
    "def modelD_inference(model, model_emb, beta_ins, diffusion, data, mask, seed, offsets):\n",
    "    s_t = time.time()\n",
    "    model.eval()\n",
    "    model_emb.eval()\n",
    "    with th.no_grad():\n",
    "        set_seed(seed)\n",
    "        x_start = model.get_embeds(data)\n",
    "        noise = th.randn_like(x_start)\n",
    "        original_mask = mask.clone()\n",
    "        mask = th.broadcast_to(mask.unsqueeze(dim=-1), x_start.shape).to(dev())\n",
    "        x_noised = th.where(mask == 0, x_start, noise)\n",
    "\n",
    "        sample_fn = (\n",
    "            diffusion.p_sample_loop if not use_ddim else diffusion.ddim_sample_loop\n",
    "        )\n",
    "        # sample_shape -> (batch, seq len, emb dim) \n",
    "        sample_shape = (x_start.shape[0], seq_len, hidden_dim)\n",
    "        \n",
    "        samples = sample_fn(\n",
    "            model,\n",
    "            sample_shape,\n",
    "            offsets,\n",
    "            noise=x_noised,\n",
    "            clip_denoised=clip_denoised,\n",
    "            denoised_fn=partial(denoised_fn_round, model_emb.cuda()),\n",
    "            top_p=top_p,\n",
    "            clamp_step=clamp_step,\n",
    "            clamp_first=True,\n",
    "            mask=mask,\n",
    "            x_start=x_start,\n",
    "            gap=step_gap,\n",
    "            # beta_ins=beta_ins\n",
    "        )\n",
    "\n",
    "        sample = samples[0]\n",
    "        del x_start, noise, samples, mask, x_noised\n",
    "        gc.collect\n",
    "        x_t = th.tensor(sample).cuda()\n",
    "        logits = model.get_logits(x_t)\n",
    "        cands = th.topk(logits, k=1, dim=-1)\n",
    "        inference_token = cands.indices\n",
    "        inference_token = inference_token.view((inference_token.shape[0], inference_token.shape[1]))\n",
    "        del x_t, logits, cands\n",
    "        gc.collect\n",
    "\n",
    "        word_lst_recover = []\n",
    "        word_lst_source = []\n",
    "        word_lst_ref = []\n",
    "        for i in range(len(inference_token)):\n",
    "            len_x = seq_len - sum(original_mask[i])\n",
    "            tokens = tokenizer.decode_token(inference_token[i][len_x:])\n",
    "            word_lst_recover.append(tokens)\n",
    "            word_lst_source.append(tokenizer.decode_token(data[i][:len_x]))\n",
    "            word_lst_ref.append(tokenizer.decode_token(data[i][len_x:]))\n",
    "        del original_mask, inference_token\n",
    "        gc.collect\n",
    "        th.cuda.empty_cache()\n",
    "    e_t = time.time() - s_t\n",
    "    write_time_usage(f'[Model D Inference] batch:{data.shape[0]}. Using {e_t/60:.2f} mins.')\n",
    "    return word_lst_recover, word_lst_source, word_lst_ref\n",
    "\n",
    "def save_model(model, model_name, master_params, step = None):\n",
    "    if step == None:\n",
    "        filename = f'model/{model_name}.pt'\n",
    "    else:\n",
    "        filename = f'model/{model_name}_{step}.pt'\n",
    "    state_dict = model.state_dict()\n",
    "    for i, (name, _value) in enumerate(model.named_parameters()):\n",
    "        assert name in state_dict\n",
    "        state_dict[name] = master_params[i]\n",
    "    msg = 'writing to', bf.join(checkpoint_path, filename)\n",
    "    write_log(msg)\n",
    "    with bf.BlobFile(bf.join(checkpoint_path, filename), \"wb\") as f: # DEBUG **\n",
    "        th.save(state_dict, f)\n",
    "        \n",
    "def get_gpu_memory():\n",
    "    command = \"nvidia-smi --query-gpu=index,memory.used --format=csv\"\n",
    "    memory_free_info = sp.check_output(command.split()).decode('ascii').split('\\n')[:-1][1:]\n",
    "    # memory_usage = memory_free_info[0].split(', ')[1]\n",
    "    return memory_free_info\n",
    "\n",
    "def lineNotifyMessage(token, msg):\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer \" + token, \n",
    "        \"Content-Type\" : \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "    \n",
    "    payload = {'message': msg}\n",
    "    r = requests.post(\n",
    "        \"https://notify-api.line.me/api/notify\",\n",
    "        headers = headers,\n",
    "        params = payload\n",
    "    )\n",
    "    return r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19dd24c",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc77eb",
   "metadata": {},
   "source": [
    "## tokenizer and model_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81308517",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2024-04-25-10-09-36-051597\n",
      "initializing the random embeddings Embedding(30522, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 128)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_dist() # Setup a distributed process group. \n",
    "logger.configure()\n",
    "\n",
    "tokenizer, vocab_size = load_tokenizer(config_name, checkpoint_path, vocab)\n",
    "model_weight = load_model_emb(vocab_size, hidden_dim, checkpoint_path)\n",
    "model_weight.to(dev())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed8f0ae",
   "metadata": {},
   "source": [
    "## initial model $B$ (encoder-decoder), model $f_\\theta$, and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "868aeccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "micro_batch_size = 32 # max 32 for model D\n",
    "N = 1 # each sentence with N betas\n",
    "\n",
    "# epochs = 40000\n",
    "best_D_loss = 999\n",
    "# best_score = 0 # using bert-score\n",
    "\n",
    "S = 1\n",
    "# valid_seed = 102\n",
    "# test_seed = []\n",
    "# while(len(test_seed) < S):\n",
    "#     seed = random.randint(0, 10000)\n",
    "#     if seed not in test_seed: test_seed.append(seed)\n",
    "\n",
    "step = diffusion_steps\n",
    "top_p = -1\n",
    "clamp_step = 0\n",
    "clip_denoised = False\n",
    "\n",
    "if step == diffusion_steps:\n",
    "    use_ddim = False\n",
    "    step_gap = 1\n",
    "else:\n",
    "    use_ddim = True\n",
    "    step_gap = diffusion_steps//step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fffc58",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e034182f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function helper_tokenize.<locals>.tokenize_function at 0x7ff7e5c751f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## \n",
      "Loading text data...\n",
      "############################## \n",
      "Loading dataset CC from ./datasets/CC...\n",
      "### Loading form the TEST set...\n",
      "### Data samples...\n",
      " ['What mineral holds the highest electrical charge?', 'Why nobody answer my questions in Quora?'] ['What mineral can hold the greatest electrical charge?', 'Why is no one answering my questions in Quora?']\n",
      "RAM used: 4340.92 MB\n",
      "Dataset({\n",
      "    features: ['src', 'trg'],\n",
      "    num_rows: 128\n",
      "})\n",
      "RAM used: 4342.78 MB\n",
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bee815cb4e410b98ad0f445009b141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddebdf078faa436dae3b94999967c825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451c81b61b7343d49268ddd23098a6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e79ac9052ec4d4ba178e6f633140ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset #3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### tokenized_datasets Dataset({\n",
      "    features: ['input_id_x', 'input_id_y'],\n",
      "    num_rows: 128\n",
      "})\n",
      "### tokenized_datasets...example [101, 2054, 9754, 4324, 1996, 3284, 5992, 3715, 1029, 102]\n",
      "RAM used: 4345.71 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e640a787910e4b4a8cb0d42f5874fe92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merge and mask:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM used: 4348.09 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14dc885b0fd94954985fea7a90910215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "padding:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],\n",
      "    num_rows: 128\n",
      "}) padded dataset\n",
      "RAM used: 4349.68 MB\n",
      "RAM used: 4349.68 MB\n"
     ]
    }
   ],
   "source": [
    "data = load_data_text(\n",
    "    batch_size=batch_size,\n",
    "    seq_len=seq_len,\n",
    "    deterministic=True,\n",
    "    dataset = dataset,\n",
    "    data_dir = data_dir,\n",
    "    loaded_vocab=tokenizer,\n",
    "    model_emb=model_weight, # use model's weights as init\n",
    "    split = 'test',\n",
    "    loop=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11d727ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_hidden, batch = next(data)\n",
    "test_data = batch['input_ids'].to(dev())\n",
    "test_mask = batch['input_mask'].to(dev())\n",
    "test_data_x = test_data * (1 - test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c00c1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {\n",
    "    'F': 0,\n",
    "    'T': 1,\n",
    "    'BOS': 2,\n",
    "} \n",
    "\n",
    "input_size = vocab_size\n",
    "output_size = 2 # T/F\n",
    "encoder_embedding_dim = 64\n",
    "decoder_embedding_dim = 64\n",
    "encoder_hidden_dim = 64\n",
    "decoder_hidden_dim = 64\n",
    "\n",
    "standard_betas = get_standard_betas(diffusion_steps, noise_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c0badd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adcc9a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BalaGinjo_checkpoint/CC/metaBeta_0422_Rsparse_e1_updateD100_FSM_reward0.2/model/modelD_30000.pt\n",
      "reload the random embeddings Embedding(30522, 128)\n",
      "Epoch: 30000, ema: False. Using 24.76 mins.\n",
      "BalaGinjo_checkpoint/CC/metaBeta_0422_Rsparse_e1_updateD100_FSM_reward0.2/model/modelD_ema_30000.pt\n",
      "reload the random embeddings Embedding(30522, 128)\n",
      "Epoch: 30000, ema: True. Using 25.77 mins.\n",
      "BalaGinjo_checkpoint/CC/metaBeta_0422_Rsparse_e1_updateD100_FSM_reward0.2/model/modelD_31000.pt\n",
      "reload the random embeddings Embedding(30522, 128)\n",
      "Epoch: 31000, ema: False. Using 24.81 mins.\n",
      "BalaGinjo_checkpoint/CC/metaBeta_0422_Rsparse_e1_updateD100_FSM_reward0.2/model/modelD_ema_31000.pt\n",
      "reload the random embeddings Embedding(30522, 128)\n",
      "Epoch: 31000, ema: True. Using 25.53 mins.\n"
     ]
    }
   ],
   "source": [
    "epochs = [i*1000 for i in range(80,80+1)]\n",
    "use_ema_list = [False, True]\n",
    "test_seed = [104]\n",
    "\n",
    "for epoch in epochs:\n",
    "    for use_ema in use_ema_list:\n",
    "        start_time = time.time()\n",
    "        # TODO: model B\n",
    "        encoder = Encoder(input_size, encoder_embedding_dim, encoder_hidden_dim).to(dev())\n",
    "        decoder = Decoder(output_size, decoder_embedding_dim, decoder_hidden_dim).to(dev())\n",
    "\n",
    "        model_path = f'{checkpoint_path}/model/encoder_{epoch}.pt'\n",
    "        encoder.load_state_dict(load_state_dict(model_path))\n",
    "        model_path = f'{checkpoint_path}/model/decoder_{epoch}.pt'\n",
    "        decoder.load_state_dict(load_state_dict(model_path))\n",
    "        \n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        beta_ins = modelB_inference(test_data_x, encoder, decoder, diffusion_steps, word2idx, 'multinomial')\n",
    "        betas = calculate_my_betas(beta_ins, standard_betas)\n",
    "        \n",
    "#         # beta schedule\n",
    "#         standard_betas = get_standard_betas(diffusion_steps, noise_schedule)\n",
    "#         # beta_ins = np.ones((batch_size, diffusion_steps-1))\n",
    "#         beta_ins = np.array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
    "#         0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
    "#         1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
    "#         0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
    "#         0, 0, 0]])\n",
    "#         beta_ins = np.repeat(beta_ins, batch_size, axis = 0)\n",
    "#         betas = calculate_my_betas(beta_ins, standard_betas)\n",
    "\n",
    "#         betas = th.tensor([0.09141289]).repeat(128, diffusion_steps)\n",
    "\n",
    "        if not timestep_respacing:\n",
    "            timestep_respacing = [diffusion_steps]\n",
    "        diffusion = SpacedDiffusion(\n",
    "            use_timesteps=space_timesteps(diffusion_steps, timestep_respacing),\n",
    "            betas=betas,\n",
    "            rescale_timesteps=rescale_timesteps,\n",
    "            predict_xstart=predict_xstart,\n",
    "            learn_sigmas = learn_sigma,\n",
    "            sigma_small = sigma_small,\n",
    "            use_kl = use_kl,\n",
    "            rescale_learned_sigmas=rescale_learned_sigmas\n",
    "        )\n",
    "\n",
    "        # model D\n",
    "        modelD = TransformerNetModel(\n",
    "            input_dims=hidden_dim,\n",
    "            output_dims=(hidden_dim if not learn_sigma else hidden_dim*2),\n",
    "            hidden_t_dim=hidden_t_dim,\n",
    "            dropout=dropout,\n",
    "            config_name=config_name,\n",
    "            vocab_size=vocab_size,\n",
    "            init_pretrained=use_plm_init\n",
    "        )\n",
    "        modelD.to(dev())\n",
    "\n",
    "        if use_ema:\n",
    "            model_path = f'{checkpoint_path}/model/modelD_ema_{epoch}.pt'\n",
    "        else:\n",
    "            model_path = f'{checkpoint_path}/model/modelD_{epoch}.pt'\n",
    "        modelD.load_state_dict(load_state_dict(model_path))\n",
    "        print(model_path)\n",
    "\n",
    "        model_emb = load_model_emb(vocab_size, hidden_dim, checkpoint_path)\n",
    "\n",
    "        model_emb.weight = th.nn.Parameter(modelD.word_embedding.weight.clone().cpu())\n",
    "        model_emb_copy = get_weights(model_emb, emb_scale_factor)\n",
    "        model_emb_copy.to(dev())\n",
    "        \n",
    "        modelD.eval()\n",
    "        for j in range(len(test_seed)):\n",
    "            # initial log file\n",
    "            if use_ema:\n",
    "                log_file_name = [\n",
    "                    f'inferences/test_inferences_ema_{epoch}',\n",
    "                    # f'inferences/test_inferences_ema_{epoch}_seed{test_seed[j]}',\n",
    "                ]\n",
    "            else:\n",
    "                log_file_name = [\n",
    "                    f'inferences/test_inferences_{epoch}',\n",
    "                    # f'inferences/test_inferences_{epoch}_seed{test_seed[j]}',\n",
    "                ]\n",
    "            for filename in log_file_name: create_log_file(filename)\n",
    "            \n",
    "            with th.no_grad():\n",
    "                loop = ((test_data.shape[0]-1) // micro_batch_size)+1\n",
    "                recovers = []\n",
    "                sources = []\n",
    "                references = []\n",
    "                for i in range(loop):\n",
    "                    batch_recovers, batch_sources, batch_references = modelD_inference(\n",
    "                        modelD, \n",
    "                        model_emb_copy,\n",
    "                        beta_ins[0],\n",
    "                        diffusion, \n",
    "                        test_data[i*micro_batch_size:(i+1)*micro_batch_size], \n",
    "                        test_mask[i*micro_batch_size:(i+1)*micro_batch_size],\n",
    "                        test_seed[j],\n",
    "                        i, # offsets\n",
    "                    )\n",
    "                    if i == 0:\n",
    "                        recovers = batch_recovers\n",
    "                        sources = batch_sources\n",
    "                        references = batch_references\n",
    "                    else:\n",
    "                        recovers = recovers+batch_recovers\n",
    "                        sources = sources+batch_sources\n",
    "                        references = references+batch_references\n",
    "\n",
    "                    del batch_recovers, batch_sources, batch_references\n",
    "                    gc.collect\n",
    "                    th.cuda.empty_cache()\n",
    "\n",
    "                # record validation inference\n",
    "                if use_ema:\n",
    "                    filename = f'inferences/test_inferences_ema_{epoch}'\n",
    "                    # filename = f'inferences/test_inferences_ema_{epoch}_seed{test_seed[j]}'\n",
    "                else:\n",
    "                    filename = f'inferences/test_inferences_{epoch}'\n",
    "                    # filename = f'inferences/test_inferences_{epoch}_seed{test_seed[j]}'\n",
    "                msg = f'Epoch {epoch} / -'\n",
    "                record_information(checkpoint_path, msg, filename)\n",
    "                for i in range(len(recovers)):\n",
    "                    msg = f'''\n",
    "                    recover:{recovers[i]},\n",
    "                    source:{sources[i]},\n",
    "                    reference:{references[i]}\n",
    "                    '''\n",
    "                    record_information(checkpoint_path, msg, filename)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f'Epoch: {epoch}, ema: {use_ema}. Using {elapsed_time / 60:.2f} mins.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
